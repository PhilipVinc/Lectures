{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc1a161",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ASC School: \"Physics meets Artificial Intelligence\"\n",
    "\n",
    "## Tutorial: Variational Monte Carlo with neural quantum states for the Dynamics of the transverse-field Ising model\n",
    "\n",
    "Filippo Vicentini, EPFL\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhilipVinc/Lectures/blob/main/2209_Munich/Dynamics.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae721781-2989-46c5-a92b-ca528b145e6c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "In this Tutorial we will introduce the open-source package [NetKet](https://www.netket.org/), and show some of its functionalities. \n",
    "As this week has already been packed and you have already seen several advanced applications of Neural Networks Quantum States (With Roger Melko), we will give a brief overview of the fundamentals of NetKet, then move on how to determine the ground state of an Hamiltonian, and finally how to simulate the dynamics.\n",
    "\n",
    "If you want to see a more 'slow-paced' and _in-depth_ tutorial only on determining the Ground State of an Hamiltonian, and about the different performance characteristics of different architectures, you can have a look at the tutorials on [NetKet's website](https://netket.readthedocs.io/en/latest/tutorials/gs-ising.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb57545-48dd-4614-8464-a589c44c10cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In This tutorial, we will study a Quench from the ground state of the 1-Dimensional Transverse-Field Ising Model for transverse field $ h=1 $ to transverse field $ h=2 $. \n",
    "\n",
    "$$ \n",
    "\\mathcal{H}=\\Gamma\\sum_{i}\\sigma_{i}^{(x)}+V\\sum_{i}\\sigma_{i}^{(z)}\\sigma_{i+1}^{(z)}. \n",
    "$$\n",
    "\n",
    "In the following we assume periodic boundary conditions and we will count lattice sites starting from $ 0 $, such that $ i=0,1\\dots L-1 $ and $i=L=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdc3a3-68f5-4d2a-9727-ed99a122f345",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "In this tutorial we will consider only $L=10$ sites so that simulations run fast. However, you can easily scale those simulations up if you use a GPU or if you have some extra time. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225eaa8d-126a-4f3f-bcd2-936780e91d4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 0. NetKet\n",
    "\n",
    "Go to [http://netket.org](https://netket.org)\n",
    "\n",
    "<img src=\"netket_web.png\" style=\"margin-left:auto; margin-right:auto\" width=\"700\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bddf19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 0a. Installing Netket \n",
    "\n",
    "If you are executing this notebook on Colab, you will need to install NetKet (and some other dependencies). You can do so by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640921c-e257-4735-8a02-fbd40891fe04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --quiet --upgrade netket==3.5 matplotlib qutip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ddb96-9d31-423a-8cfa-0cb42a1ce094",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b> Running on GPUs: </b>\n",
    "\n",
    "You need a Linux computer and a Nvidia/AMD Gpu, and install the correct version of Jax.\n",
    "On Colab, this is already installed if you are running in a GPU instance.\n",
    "    \n",
    "```\n",
    "pip install --upgrade -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html 'jax[cuda]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15de72d-9454-4873-a88f-a02ec10459ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 0b. Running NetKet\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "We also want make to sure that this notebook is running on the cpu. \n",
    "You can edit the field by changing \"cpu\" to \"gpu\" to make it run on the GPU if you want. \n",
    "    \n",
    "But you'll need to use much larger systems to see a benefit in the runtime.\n",
    "For systems with less than 40 spins GPUs slow you down remarkably.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752288d-caf4-4f53-a104-d4b2efe6e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb06b93-5fb0-47d5-be6d-d05f2e00c025",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "You can check that the installation was succesfull doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddace47-7d2b-4d2d-8822-dc0755425e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk\n",
    "print(nk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3541eed-39c2-40dd-bce7-4364061fe32f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### 0c. Import some useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add39c1-4c5c-40b1-9739-b27785ed9640",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numerical operations in the model should always use jax.numpy \n",
    "# instead of numpy because jax supports computing derivatives. \n",
    "# If you want to better understand the difference between the two, check\n",
    "# https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Flax is a framework to define models using jax\n",
    "import flax\n",
    "# we refer to `flax.linen` as `nn`. It's a repository of \n",
    "# layers, initializers and nonlinear functions.\n",
    "import flax.linen as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7effe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Defining the system and  Hamiltonian\n",
    "\n",
    "The first step in our journey consists in defining the Hamiltonian we are interested in. \n",
    "For this purpose, we first need to define the kind of degrees of freedom we are dealing with (i.e. if we have spins, bosons, fermions etc). \n",
    "\n",
    "This is done specifying the Hilbert space of the problem. For example, let us concentrate on a problem with 10 spins-1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5adc02-f048-4617-bdea-aba818273296",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1b. Define The computational basis (Hilbert space)\n",
    "\n",
    "When building hilbert spaces, in general, the first argument determines the size of the local basis and the latter defines how many modes you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5273d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import netket as nk\n",
    "\n",
    "N = 10\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe4b95-17c9-40ec-bdfc-bc5d5bbd32bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "NetKet's Hilbert spaces define the computational basis of the calculation, and are used to label and generate elements from it. \n",
    "The standard Spin-basis implicitly selects the `z` basis and elements of that basis will be elements $ v\\in\\{\\pm 1\\}^N $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d889a00-c71a-450e-8ba5-af5c693706a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It is possible to generate random basis elements through the function `random_state(rng, shape, dtype)`, where the first argument must be a jax RNG state (usually built with `jax.random.PRNGKey(seed)`, second is an integer or a tuple giving the shape of the samples and the last is the dtype of the generated states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bf39c-ae7f-4cb0-b141-093c448138e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "hi.random_state(jax.random.PRNGKey(0), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e67ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1c. Define the Hamiltonian\n",
    "\n",
    "We now need to specify the Hamiltonian. For this purpose, we will use NetKet's ```LocalOperator``` (see details [here](https://www.netket.org/docs/_generated/operator/netket.operator.LocalOperator.html#netket.operator.LocalOperator)) which is the sum of arbitrary k-local operators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e62105-1bd2-4fe4-9d07-502748fe96d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In this specifc case, we have a 1-local operator, $ \\sigma^{(x)}_i $ and a 2-local operator, $ \\sigma^{(z)}_i \\sigma^{(z)}_j $. We then start importing the pauli operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netket.operator.spin import sigmax, sigmaz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c35665",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We now take $ \\Gamma=-1 $ and start defining the 1-local parts of the Hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = -1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41d740",
   "metadata": {},
   "source": [
    "Here we have used a list comprehension to (mildly) show off our ability to write one-liners, however you could have just added the terms one by one in an explicit loop instead (though you'd end up with a whopping 3 lines of code). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2f6a5-b40b-46a8-82f1-7e8d102b0538",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We now also add the interaction terms, using the fact that NetKet automatically recognizes products of local operators as tensor products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378d9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Gamma = -1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])\n",
    "print(\"Local terms: \\n\", H)\n",
    "\n",
    "V=-1\n",
    "H += sum([V*sigmaz(hi,i)*sigmaz(hi,(i+1)%N) for i in range(N)])\n",
    "print(\"Local and Interaction terms: \\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59025d8c-dede-496f-b2f0-6ad63966be1d",
   "metadata": {},
   "source": [
    "In general, when manipulating NetKet objects, you should always assume that you can safely operate on them like \n",
    "you would in mathematical equations, therefore you can sum and multiply them with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52532b45-4c1a-4cf9-b730-d294f7400e99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1d. Special Hamiltonians\n",
    "\n",
    "For some specific Hamiltonians with a very specific structure, such as the Ising Hamiltonian, we can use a 'custom-made' implementation that is more performant. This implementation can only represent the Ising hamiltonian, and is of course less flexible than the mechanism above which allows us to write any (Local) hamiltonian.\n",
    "\n",
    "To use this specific implementation, you need to define the graph over which we defined the model. In our case, that is the 1D periodic chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a926c6-39e9-4f0e-bb08-2d6e0e0f37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = nk.graph.Chain(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0ac6b-e76e-46dd-bea6-77b15c7a6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eba26-6050-4673-9eb2-75641753519c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also print it, or plot it in order to inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec71ad7-457e-4f04-a0ae-35510b7d9edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lattice.draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6f9b2-38b0-47d0-93db-923433cebb6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "And we can now build the custom Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222b826-7cee-4f5a-8455-1adbe16a71a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "?nk.operator.Ising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2c55d-3af3-47ce-afee-0eb8feeb9d87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "H_special = nk.operator.Ising(hi, lattice, h=1.0, J=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406282e-40dd-4cdf-8e39-3db41b7c695c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And we can compare the two: if we check if they are equal, they are not, \n",
    "# because they are different implementations\n",
    "H_special == H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4db51-840d-490f-bec8-ca0d5cad11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we compare their matrix versions, they do match:\n",
    "import numpy as np\n",
    "np.allclose(H_special.to_dense(), H.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea8ae8-a589-4d13-a7db-1f8dd05c7ca4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also get the Group of Translations or other symmetries of this lattice, if you're into group theory and simmetries...  but we won't be talking about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c8d06-08c6-45f1-873e-44689dba1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice.translation_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65581d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. Exact Diagonalization\n",
    "\n",
    "Now that we have defined the Hamiltonian, we can already start playing with it. For example, since the number of spins is large but still manageable for exact diagonalization, we can give it a try. \n",
    "\n",
    "In NetKet this is easily done converting our Hamiltonian operator into a sparse matrix of size $ 2^N \\times 2^ N $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h=H.to_sparse()\n",
    "sp_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b786e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Since this is just a regular scipy sparse matrix, we can just use any sparse diagonalization routine in there to find the eigenstates. For example, this will find the two lowest eigenstates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "eig_vals, eig_vecs = eigsh(sp_h, k=2, which=\"SA\")\n",
    "\n",
    "print(\"eigenvalues with scipy sparse:\", eig_vals)\n",
    "\n",
    "E_gs = eig_vals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51cc79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Variational States: A RBM-NQS\n",
    "\n",
    "In NetKet one has to define a variational function approximating the **logarithm** of the wave-function amplitudes (or density-matrix values).\n",
    "We call this variational function _the Model_ (yes, caps on the M).\n",
    "\n",
    "$$ \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{mf}} \\rangle = \\exp\\left[\\mathrm{Model}(\\sigma^{z}_1,\\dots \\sigma^{z}_N ; \\theta ) \\right], $$\n",
    "\n",
    "where $\\theta$ is a set of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948ddd4-5806-43fb-bdc6-51cddf1f99d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The Model can be defined using one of the several *functional* jax frameworks such as Jax/Stax, Flax or Haiku. \n",
    "NetKet includes several pre-built models and layers built with [Flax](https://github.com/google/flax), so we will be using it for the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e635277-c3ca-41af-9757-c9139cb5741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only creates the 'definition' of the model, not the parameters themselves.\n",
    "model = nk.models.RBM(alpha=1, param_dtype=complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b0123-8097-46a4-b4c8-d050a974f400",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The model itself is only a set of instructions on how to initialise the parameters and how to compute the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441b547-addf-4315-9f15-094fb970623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To initialize the parameters, you provide some 'mock' inputs:\n",
    "pars = model.init(jax.random.PRNGKey(0), hi.numbers_to_states(np.arange(3)))\n",
    "\n",
    "# and then we can evaluate it:\n",
    "input = hi.random_state(jax.random.PRNGKey(3), 10)\n",
    "logpsi_val = model.apply(pars, input)\n",
    "print(f\"An input of shape {input.shape=} gives output {logpsi_val.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ca947-9312-4de1-bbb8-2a5e112c8cba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3a. Constructing the Variational State\n",
    "\n",
    "To actually create a variational state with its parameters, the easiest way is to construct a Monte-Carlo-sampled Variational State. \n",
    "To do this, we first need to define a sampler.\n",
    "\n",
    "In `netket.sampler` several samplers are defined, each with its own peculiarities. \n",
    "In the following example, we will be using a simple sampler that flips the spins in the configurations one by one.\n",
    "\n",
    "You can read more about how the sampler works by checking the documentation with `?nk.sampler.MetropolisLocal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb0ac3-bc4c-4743-adec-03bb4e2d7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local sampler on the hilbert space\n",
    "sampler = nk.sampler.MetropolisLocal(hi, n_chains=8, n_sweeps=1)\n",
    "\n",
    "# Construct the variational state using the model and the sampler above.\n",
    "# n_samples specifies how many samples should be used to compute expectation\n",
    "# values.\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=20*4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9925a2-fbfb-4580-bba9-d8e233ab798c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "You can play around with the variational state: for example, you can compute expectation values yourself or inspect it's parameters.\n",
    "The parameters are stored as a set of nested dictionaries. In this case, the single parameter $\\lambda$ is stored inside a (frozen) dictionary.\n",
    "(The reason why the dictionary is frozen is a detail of Flax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fae25-6b39-41c1-a25e-51de791b7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vstate)\n",
    "print(vstate.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f23399-7a3c-43c1-8d1c-14f8d5f13ea3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3b. Computing expectation values\n",
    "\n",
    "The **AllMighty** Giuseppe Carleo has shown you that, given a variational state\n",
    "\n",
    "$$\n",
    "\\vert\\psi_\\theta\\rangle = \\sum_\\sigma \\psi_\\theta(\\sigma)\\vert\\sigma\\rangle\n",
    "$$\n",
    "\n",
    "We can estimate expectation values in Polynomial time by writing them as:\n",
    "\n",
    "\\begin{align}\n",
    "\\langle \\psi_\\theta\\vert \\hat{H}\\vert\\psi_\\theta \\rangle &= \\sum_\\sigma \\langle \\psi_\\theta\\vert\\sigma\\rangle\\langle\\sigma\\vert \\hat{H}\\vert\\psi_\\theta \\rangle \\\\\n",
    "&=  \\sum_\\sigma \\langle \\psi_\\theta\\vert\\sigma\\rangle\\langle\\sigma\\vert\\psi_\\theta\\rangle\\frac{\\langle\\sigma\\vert \\hat{H}\\vert\\psi_\\theta \\rangle}{\\langle\\sigma\\vert\\psi_\\theta\\rangle} \\\\\n",
    "&=  \\sum_\\sigma |\\psi_\\theta(\\sigma)|^2H^{\\text{Loc}}(\\sigma) \\\\\n",
    "&= \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[H^{\\text{Loc}}(\\sigma)\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b5a7b-9c70-4834-b826-bb95acc18131",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's generate the samples\n",
    "σ = vstate.sample()\n",
    "print(f\"My samples have {σ.shape = } which comes from {vstate.sampler.n_chains = } \",\n",
    "      \"chains of 10'000 samples each, and an hilbert space of {hi.size=} spins.\")\n",
    "\n",
    "# we can also inspect the acceptance rate:\n",
    "print(\"The acceptance rate right now is stored in the `sampler_state`: \", vstate.sampler_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad4d52-fc97-4e8b-930f-ec0fbf112169",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's generate the local energies\n",
    "H_loc = vstate.local_estimators(H)\n",
    "print(f\"My local estimators have a shape of {H_loc.shape = }. The samples are taken from `vstate.samples`, which are the last sampled samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17cf86-eae6-4efe-a2cf-c54f26f6b492",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's compute the expectation value:\n",
    "print(\"<H> =\", jnp.mean(H_loc), \"Var[H] = \", jnp.var(H_loc), \"\\n\")\n",
    "\n",
    "# But a much better way to compute it would be: \n",
    "nk.stats.statistics(H_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd825183-e390-4f05-9bb5-ffdd0e5d4629",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "With a variational state, you can compute expectation values directly of operators using `vstate.expect`. For our `MCState`, this internally performs Monte Carlo sampling over `n_samples` spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea20c03-dcfa-4cda-8f2f-b1d86a95872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstate.n_samples = 4000\n",
    "E = vstate.expect(H)\n",
    "print(type(E))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e0116-93f7-4816-9b0a-85ebdd880bce",
   "metadata": {},
   "source": [
    "The return value of `.expect` is an object of type `Stats` which, beside the energy expectation value contains some statistics of the ensemble of local energies over which the energy is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5973c-77ad-4c3a-b1a4-f68d91ebc81d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The `Stats` object also has a dictionary representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987ca3b-f4ec-4448-9304-27f3c2840350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(E.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703600e4-0df8-4b27-b3f4-aba1eb71d72e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The `Stats` object contains the following diagnostics:\n",
    "\n",
    "* The `Mean` over the local energy samples, which is an estimate of the quantum expectation value $\\langle \\hat H \\rangle.$\n",
    "\n",
    "* The `Variance` over the local energy samples, which is an estimate of the quantum variance $\\langle(\\delta\\hat H)^2\\rangle = \\langle (\\hat H - \\langle \\hat H \\rangle)^2 \\rangle.$\n",
    "\n",
    "* The Monte Carlo standard error (MCSE) of the mean (as `Sigma`).\n",
    "\n",
    "* An estimate `TauCorr` of the autocorrelation time over the Markov chains.\n",
    "\n",
    "* `R_hat`, which is the so-called Gelman-Rubin split-R_hat diagnostic, which indicates whether the MCMC chain is converged (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfba40-5626-45cd-8460-71077d23e15d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If you are close to an eigenstate of the operators, the variance should be close to 0. (In an exact eigenstate, the local energy is constant and therefore the variance is exactly zero.)\n",
    "\n",
    "The Gelman-Rubin diagnostic will be $\\hat{R}\\approx 1$ if the Markov chains are converged, while it will be larger than $1$ if your sampling has not converged.\n",
    "As a rule of thumb, look out for $|\\hat{R}| > 1.1$, and if that occurs consistently, check if you need more samples or if you MCMC scheme is even consistent with your system. (This is a somewhat weak criterion. Modern MCMC literature tends to recommend $|\\hat{R}| > 1.01$ to discard a sample. NQS optimization can still work with the less stringent criterion, but if you see stability problems, keep this in mind.)\n",
    "\n",
    "You can also investigate the correlation time of your estimator, $\\tau$. If $\\tau\\gg1$ then your samples are very correlated and you most likely have some issues with your sampling scheme.\n",
    "\n",
    "You can also access the fields individually:\n",
    "Note that if you run your calculation using MPI on different processes/machines, those estimators will return the mean, error and estimators of all the samples across all the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db41d8f-fae4-4ba1-b526-6161a127cd21",
   "metadata": {},
   "source": [
    "### 3c. Computing gradients of Expectation values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd8c5d-af97-4683-88a6-6a4dba9decb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Above we have computed the expectation value as:\n",
    "\n",
    "\\begin{align}\n",
    "\\langle \\psi_\\theta\\vert \\hat{H}\\vert\\psi_\\theta \\rangle &=   \\sum_\\sigma |\\psi_\\theta(\\sigma)|^2H^{\\text{Loc}}(\\sigma) \\\\\n",
    "&= \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[H^{\\text{Loc}}(\\sigma)\\right]\n",
    "\\end{align}\n",
    "\n",
    "But... in order to use an optimisation algorithm such as gradient descent, we will need to compute the gradient of this quantity as well. \n",
    "For an expectation value, it is easy to derive the formula:\n",
    "\n",
    "$$\n",
    "    \\nabla_\\theta \\mathbb{E}_{X\\sim p_\\theta(X)}\\left[ f_\\theta(X) \\right] = \\mathbb{E}_{X\\sim p_\\theta(X)}\\left[\\nabla_\\theta f_\\theta(X) \\right]- \\mathbb{E}_{X\\sim p_\\theta(X)}\\left[(\\nabla_\\theta\\log\\psi_\\theta(X))f_\\theta(X) \\right]\n",
    "$$\n",
    "\n",
    "Which, translated to our case, gives:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\langle \\psi_\\theta\\vert \\hat{H}\\vert\\psi_\\theta \\rangle = \\\\\n",
    "&= 2\\Re\\left\\{\\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[(\\nabla_\\theta\\log\\psi(\\sigma)) H^{\\text{Loc}}(\\sigma)\\right] - \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[\\nabla_\\theta\\log\\psi(\\sigma)\\right] \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[H^{\\text{Loc}}(\\sigma)\\right] \\right\\} \\,\\,\\,\\, &&\\text{If parameters are real} \\\\\n",
    "&= \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[(\\nabla_\\theta\\log\\psi(\\sigma)) H^{\\text{Loc}}(\\sigma)\\right] - \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[\\nabla_\\theta\\log\\psi(\\sigma)\\right] \\mathbb{E}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}\\left[H^{\\text{Loc}}(\\sigma)\\right] \\,\\,\\,\\, &&\\text{If parameters are complex}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10eed3f-ae19-4250-bc68-135559d837dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now, as we showed before, the parameters in NetKet are not stored as a single array/vector $\\vec{\\theta} = \\{\\theta_1, \\theta_2, \\dots\\theta_N\\}$, but rather as a PyTree, which is a set of nested dictionaries containing arbirarily shaped matrices, tensors and scalars which, overall, behave as arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ef1e1-a15c-4448-9c0e-5527a19fdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_map(lambda x: (x.shape, x.dtype), vstate.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f986001-47ca-4713-b09a-be68f882058d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "So we will want the gradient to have the same shape...\n",
    "\n",
    "I will not show you how to do it by hand (because it's a bit involved, but you can have a look [here](https://github.com/netket/netket/blob/77a23dcdc42b63f0116510e1064e82351f830fb0/netket/vqs/mc/mc_state/expect_forces.py#L67) if you really are curious.\n",
    "Instead, NetKet allows you to compute the gradient directly and efficiently by calling the function \n",
    "\n",
    "```\n",
    "vstate.expect_and_grad(operator)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72dc58-4ac0-4ebe-a2d3-8eb28799af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, grad_E = vstate.expect_and_grad(H)\n",
    "\n",
    "jax.tree_map(lambda x: (x.shape, x.dtype), grad_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6db833-bd70-46fd-8270-80f8f3663c76",
   "metadata": {},
   "source": [
    "Lets look at how the statistics work a bit more closely. We'll be using some artifically generated random data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c35bf-1441-4b23-a02a-f330b49b6375",
   "metadata": {},
   "source": [
    "Getting back to our VMC example, let's try out a couple of `n_samples` values and see what happens to the energy statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b6302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4. Variational Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab77267-892e-48d2-97e2-eee6c8eae796",
   "metadata": {},
   "source": [
    "We will now try to optimise $ \\lambda $ in order to best approximate the ground state of the hamiltonian.\n",
    "\n",
    "At first, we'll try to do this by ourself by writing the training loop, but then we'll switch to using a pre-made\n",
    "solution provided by netket for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103e357-f079-4816-8323-d9a606748a04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 4a. DIY Optimisation loop\n",
    "\n",
    "The optimisation (or training) loop must do a very simple thing: at every iteration it must compute the energy and it's gradient, then multiply the gradient by a certain learning rate $\\lambda = 0.05$ and lastly it must update the parameters with this rescaled gradient.\n",
    "You can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b0687-1f78-4c2e-9f17-16c3fac2f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "energy_history = []\n",
    "n_steps = 100\n",
    "\n",
    "# For every iteration (tqdm is just a progress bar)\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad = vstate.expect_and_grad(H)\n",
    "    # log the energy to a list\n",
    "    energy_history.append(E.mean.real)\n",
    "    # equivalent to vstate.parameters - 0.05*E_grad , but it performs this\n",
    "    # function on every leaf of the dictionaries containing the set of parameters\n",
    "    new_pars = jax.tree_map(lambda x,y: x-0.05*y, vstate.parameters, E_grad)\n",
    "    # actually update the paramters\n",
    "    vstate.parameters = new_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22ec7f-4479-47d4-a921-b568a4280c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We now can plot the energy during those optimisation steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f2a0e-3408-443d-8f9b-13f70db15231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ade91-7ad7-4a8c-997d-e4ee9098b2d7",
   "metadata": {},
   "source": [
    "### 4b. Use NetKet's optimisation driver \n",
    "\n",
    "As writing the whole optimisation loop by yourself every time is.. boring, we can make use of a coupled of NetKet's built-in utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we reset the parameters to run the optimisation again\n",
    "vstate.init_parameters()\n",
    "\n",
    "# Then we create an optimiser from the standard library.\n",
    "# You can also use optax.\n",
    "optimizer = nk.optimizer.Sgd(learning_rate=0.1)\n",
    "\n",
    "# build the optimisation driver\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate)\n",
    "\n",
    "# Use one of the NetKet's internal loggers\n",
    "log = nk.logging.RuntimeLog()\n",
    "\n",
    "# run the driver for 300 iterations. This will display a progress bar\n",
    "# by default.\n",
    "gs.run(n_iter=300, out=log)\n",
    "\n",
    "mf_energy=vstate.expect(H)\n",
    "error=abs((mf_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",mf_energy,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5976b-da47-45af-88e8-690afb3980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25c8c6-4e96-43e4-990c-d4e312831f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.data['Energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b51740-983b-4bef-883c-6104248eb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(log.data['Energy'].iters, log.data['Energy'].Mean.real, yerr=log.data['Energy'].Sigma);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Energy\")\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b80a5a-7e30-4cac-ac48-b000027d90ac",
   "metadata": {},
   "source": [
    "## 5. Dynamics! \n",
    "\n",
    "Let's try to do dynamics. Starting from the ground state at $h=1$ . \n",
    "\n",
    "To be safe, let's store our starting parameters (so that we can always reset them).\n",
    "In Jax/Flax/NetKet arrays and parameters are immutable, meaning that you can **never** modify them in-place. So storing them here is safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3c4e2-3677-45fd-8240-ee86894da0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "θ_0 = vstate.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb123a29-2fc0-416c-9fa6-b210e5902dff",
   "metadata": {},
   "source": [
    "Now let's define the parameters of the simulation we want to run, so that we can generate some exact data to benchmark against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4c16c-536e-4e7d-b8bc-3b3aa53c5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_quench = nk.operator.Ising(hi, lattice, h=0.5, J=1.0)\n",
    "\n",
    "t0 = 0.0\n",
    "t_end = 2.0\n",
    "\n",
    "# observables\n",
    "Sx = sum([nk.operator.spin.sigmax(hi, i) for i in range(hi.size)])/hi.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d3fe1-efc1-4357-a51a-f54997a1bc92",
   "metadata": {},
   "source": [
    "### 5a. Exact solution\n",
    "\n",
    "Let's now use [QuTiP](https://qutip.org/) to solve the equation exactly.\n",
    "To convert NetKet's objects to QuTiP you can just call `.to_qobj()` on them. Remember that QuTiP scales exponentially, as it does calculations in the full Hilbert space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38193375-06cf-4e44-b186-8b928272e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qutip as qt\n",
    "\n",
    "#Schroedinger-Equation-SOLVE\n",
    "sol = qt.sesolve(H_quench.to_qobj(), vstate.to_qobj(), np.arange(t0, t_end, 0.01), e_ops=[Sx.to_qobj()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef09043-4eba-48f5-ac91-edbb8f847472",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sol.times, sol.expect[0])\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.ylabel(\"Sigma X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1b380-a8b9-472b-a166-fa5ff01cbff5",
   "metadata": {},
   "source": [
    "### 5b. TDVP\n",
    "\n",
    "Let's now use the TDVP.\n",
    "\n",
    "You have heard in the lecture by the AllMighty how to do time evolution on a variational ansatz.\n",
    "Otherwise, helpful references for the derivation of the TDVP equations of motion are, e.g, [Yuan et al. \"Theory of Variational Simulation\" (Quantum 3, 191, 2019)](https://quantum-journal.org/papers/q-2019-10-07-191/), and [Stokes et al. (arXiv:2203.14824)](https://arxiv.org/abs/2203.14824).\n",
    "\n",
    "Assuming we have the parametrized variational ansatz $\\vert\\psi_\\theta\\rangle $, where to every set of parameters $\\theta$ we associate the state $\\theta \\rightarrow \\vert\\psi_\\theta\\rangle$, determining the dyanmics will mean finding the parameters at every time $\\theta(t)$ so that\n",
    "$$\n",
    "    \\theta(t) \\rightarrow \\vert\\psi_{\\theta(t)}\\rangle\n",
    "$$\n",
    "\n",
    "Looking at a single time-step, assuming we know the parameters $\\theta(t)$, we want to determine the parameters at time $\\theta(t+dt)$. \n",
    "Those must maximise the overlap between the evolved-state $\\vert \\psi_{\\theta(t+dt)}\\rangle$ and the _exactly evolved_ $ e^{-i\\hat{H}dt}\\vert\\psi_{\\theta(t)}\\rangle$\n",
    "\n",
    "$$\n",
    "    \\max_{\\delta\\theta} |\\langle \\mathrm{e}^{-\\gamma \\hat H \\delta t} \\psi_{\\theta} | \\psi_{\\theta + \\delta\\theta} \\rangle|^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a92025-468d-4dcd-95d3-e085b577b35f",
   "metadata": {},
   "source": [
    "Taylor expanding this condition to second order in $\\delta\\theta$ and $\\delta t$ yields after some steps the equation of motion\n",
    "\n",
    "$$\n",
    "    S(\\theta) \\, \\dot\\theta = -\\gamma F(\\theta, t)\n",
    "$$\n",
    "\n",
    "with the quantum geometric tensor\n",
    "\n",
    "$$\n",
    "    S_{ij}(\\theta) = \\frac{\n",
    "       \\langle\\partial_i\\psi_\\theta | \\partial_j\\psi_\\theta\\rangle\n",
    "    }{\n",
    "       \\langle \\psi_\\theta | \\psi_\\theta \\rangle\n",
    "    } - \\frac{\n",
    "       \\langle\\partial_i\\psi_\\theta | \\psi_\\theta \\rangle\\langle \\psi_\\theta | \\partial_j\\psi_\\theta\\rangle\n",
    "    }{\n",
    "       \\langle \\psi_\\theta | \\psi_\\theta \\rangle^2\n",
    "    }\n",
    "$$\n",
    "\n",
    "and forces vector (Note: this is the same as the gradient of the expectation value if $\\theta_i\\in\\mathbb{C}$, but it **is not** the same if the parameters are Real. In particular, if parameters are real, the gradient of the expectation value is the real part of the force vector shown below)\n",
    "\n",
    "$$\n",
    "    F_i(\\theta, t) = \\frac{\\partial\\langle \\hat H \\rangle}{\\partial\\theta_i^*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705022ea-22c5-4b69-b1fb-1edf053206bf",
   "metadata": {},
   "source": [
    "$\\gamma = 1$ results in imaginary time evolution, $\\gamma = \\mathrm i$ gives real time evolution instead.\n",
    "\n",
    "$S$ and $F$ can be estimated using Monte Carlo sampling: Given $\\sigma \\sim |\\psi_\\theta(\\sigma)|^2$, we can estimate those quantities by\n",
    "$$\n",
    "    S_{ij} = \\mathbb{C}\\text{ov}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}(O_i(\\sigma), O_j(\\sigma))\n",
    "    \\qquad\n",
    "    F_i = \\mathbb{C}\\text{ov}_{\\sigma\\sim|\\psi_\\theta(\\sigma)|^2}(O_i(\\sigma), H^{\\text{Loc}}(\\sigma))\n",
    "$$\n",
    "\n",
    "where $O_j$ are the log-derivatives\n",
    "\n",
    "$$\n",
    "    O_j(\\sigma) = \\frac{\\partial\\ln\\psi_\\theta(\\sigma)}{\\partial\\theta_j}.\n",
    "$$\n",
    "\n",
    "In NetKet, the quantum geometric tensor $S_\\theta$ is available from the variational state class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55541d9-58dc-4d16-9f9d-ac9e67315810",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstate.quantum_geometric_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edc0ed-c7a2-47a5-8922-d87d00fe5a70",
   "metadata": {},
   "source": [
    "This object behaves like a matrix that can be multiplied by the vector of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4371b-75bb-4d3f-9703-46185ada54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vstate.quantum_geometric_tensor() @ vstate.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71513be6-1966-4ae7-915a-9d65008ed9ab",
   "metadata": {},
   "source": [
    "So the only thing we need is now to implement the equations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc6dbb-6a2e-4397-86a7-573190c56cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The geometric tensor\n",
    "S = vstate.quantum_geometric_tensor()\n",
    "\n",
    "# The forces vector\n",
    "E, F = vstate.expect_and_forces(H_quench)\n",
    "\n",
    "# multiply by -1j\n",
    "gamma_F = jax.tree_map(lambda x: -1j*x, F)\n",
    "\n",
    "# solve the linear system dθ = S^{-1}F\n",
    "dθ, _ = S.solve(nk.optimizer.solver.svd, gamma_F)\n",
    "\n",
    "dt = 0.01\n",
    "θ_tpdt = jax.tree_map(lambda x, y: x + dt * y, vstate.parameters, dθ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa3105-25d9-434d-8b06-d07309883175",
   "metadata": {},
   "source": [
    "Let's put this in a function that takes the current state and returns also some observable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231af1a6-13c7-4780-b4a3-8168afb42677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep(vstate, dt):\n",
    "    # The geometric tensor\n",
    "    S = vstate.quantum_geometric_tensor()\n",
    "\n",
    "    # The forces vector\n",
    "    E, F = vstate.expect_and_forces(H_quench)\n",
    "\n",
    "    # multiply by -1j\n",
    "    gamma_F = jax.tree_map(lambda x: -1j*x, F)\n",
    "\n",
    "    # solve the linear system dθ = S^{-1}F\n",
    "    dθ, _ = S.solve(nk.optimizer.solver.svd, gamma_F)\n",
    "\n",
    "    θ_tpdt = jax.tree_map(lambda x, y: x + dt * y, vstate.parameters, dθ)\n",
    "    return E, θ_tpdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2e89f-f01c-42fb-a57a-e3c195568c0b",
   "metadata": {},
   "source": [
    "And we can use this function in a loop to solve the full dynamics, storing the Energy and other quantities at every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f350b30-ac3e-424d-82a6-2dc5c61677c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t0\n",
    "vstate.parameters = θ_0\n",
    "dt = 0.005\n",
    "\n",
    "log_dynamics = nk.logging.RuntimeLog()\n",
    "\n",
    "for t in tqdm(np.arange(t0, t_end, dt)):\n",
    "    # Take one time-step\n",
    "    E_t, θ_tpdt = timestep(vstate, dt)\n",
    "    \n",
    "    # Put together a dictionary of quantities to 'log' at this timestep\n",
    "    log_data_t = {'Energy': E_t, 'Sx': vstate.expect(Sx)}\n",
    "    # Log the data\n",
    "    log_dynamics(t, log_data_t, vstate)\n",
    "    \n",
    "    # update the parameters\n",
    "    vstate.parameters = θ_tpdt\n",
    "    #t = t + dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa26f-8bb8-4280-a554-b509164f3f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3773a26-b41a-4389-a0f9-395d304fe517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_dynamics.data['Energy'].iters, log_dynamics.data['Energy'].Mean.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9542e3-21b3-473f-8872-5dc607781b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sol.times, sol.expect[0], label = \"Exact\")\n",
    "plt.plot(log_dynamics.data['Energy'].iters, log_dynamics.data['Sx'].Mean.real, label=\"RBM(alpha=1)+dt=0.005\")\n",
    "plt.ylabel(\"Sigma X\")\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1221b8-6cda-47c8-90ba-3b9aa35901b0",
   "metadata": {},
   "source": [
    "### 5c. NetKet's simpler interface\n",
    "\n",
    "Instead of writing your time-integration loop, or if you want to use higher-order ODE integrators (beyond Euler) to solve the differential equation, you can use our own TDVP interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1800f-fca1-4a3d-867a-17792143c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's in netket.experimental, because we are still not 100% sure we got the API right, but it has been stable for the last year or so\n",
    "import netket.experimental as nkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e83fb6-cb39-4b56-831b-54c53a91a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an integrator (this has order 2)\n",
    "integrator = nkx.dynamics.Heun(dt=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e2eca-359e-4efc-bfd0-384b821f0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset the parameters again\n",
    "vstate.parameters = θ_0\n",
    "\n",
    "driver = nkx.TDVP(\n",
    "    H_quench,\n",
    "    vstate,\n",
    "    integrator,\n",
    "    linear_solver=nk.optimizer.solver.svd,\n",
    "    qgt=nk.optimizer.qgt.QGTJacobianDense(holomorphic=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89060768-01fd-4762-8216-1149c47bec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = nk.logging.RuntimeLog()\n",
    "driver.run(T=1.0, obs={\"Sx\": Sx}, out=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27f7f0-974f-4afe-b453-d5ae02f63eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22063639-fe05-4128-b6e2-9f04a5e170d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Netket development",
   "language": "python",
   "name": "dev-netket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
