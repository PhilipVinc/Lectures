{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc1a161",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variational Monte Carlo and Neural Quantum States from (almost) Scratch\n",
    "\n",
    "\n",
    "Authors: Filippo Vicentini (Ã‰cole Polytechnique) and Alessandro Sinibaldi (EPFL)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhilipVinc/Lectures/blob/master/2406_LesHouches/1-tutorial_vmc.ipynb)\n",
    "\n",
    "14 June, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae721781-2989-46c5-a92b-ca528b145e6c",
   "metadata": {},
   "source": [
    "The objective of this TuTorial is to get you to write a **modern** Variational Monte Carlo code yourself, that you understand how it is built, such that you can become a great scientist in the future and build upon this new knowledge.\n",
    "To do that, we will pick a simple problem: the 2D Transverse-Field ising Model and we will try to compute its ground-state, and later on, we will try to simulate a quench. \n",
    "For your information, regardless of what people say those days, the 2D Transverse Field Ising model's ground state at the critical point (and around) can be determined with relatively simple VMC ansatze to almost numerical precision on lattices in excess of hundreds of sites with a reasonable cost (~half a day on a GPU for ~8x8, a few extra GPUs on larger lattices). \n",
    "For the 16 sites you're playing with today you should expect simulations that run in few seconds for simple networks, and ~minute(s) for more complex networks.\n",
    "\n",
    "Code wise, I'm a fan of **efficient** research. I think you can write a VMC code in C/++ or Rust and handwritten CUDA (note below), and if you're telling me that, you are surely capable of doing that. However nowdays, I prefer to be able to prototype an algorithm in 10 minutes, and let my GPU do some extra work for me, see that the algorithm does not work, and prototype the next algorithm in 10 minutes. \n",
    "\n",
    "(Note: recently Andrej Karphathy reimplemented GPT-2 in handwritten C++ and CUDA, and achieved a factor of 10 speedup over PyTorch... so there's value in handwriting things. But only if you know exactly what you want to implement! That code is an unreadable mess, and it's leveraging architecture-specific optimisations, and you cannot change the network architecture at all https://x.com/karpathy/status/1795484547267834137 )\n",
    "\n",
    "My goal is to show you that you can write **simple implementations** of many algorithms that are efficient, and you can build upon that.\n",
    "The main building block will be Automatic Differentiation, which can at times be a bit counter-intuitive, but which is extermely powerful!\n",
    "To avoid spending time on some 'tough' points that are technically challenging but which have no intrinsic physical interest, we will be building upon the open source package we develop, [NetKet](https://www.netket.org/) .\n",
    "In particular, the things that will be taken from NetKet are:\n",
    " - The Monte Carlo samplers: because writing one is not fun, but it is not hard either. If you want, you can reimplement them over the weekend!\n",
    " - The operators: Writing an efficient implementation of (arbitrary) sparse operators that can efficiently be indexed in a way that is relevant to VMC is again not straightfoward, and very error prone. I don't want you to spend time thinking about that today. Instead, we will use them as a 'black box' with a well defined interface. Our implementation is very easy to use and flexible, but again, you're free to reimplement it however you want in the future! It will just take you a few sleepless nights and many cups of coffee.\n",
    " - Complex value Automatic Differentiation fixes: AD in Pytorch does not work with complex numbers, while in Jax it does some funny, counter intuitive things (counter intuitive for physicists. Computer scientists will tell you that it makes perfect sense). We will be re-using some AD primitives from netket that make it more straightforward to work with complex numbers and translate the formulas on the blackboard to code, but if you're curious about what happens under the hood and why we need to do that, ask me!\n",
    "\n",
    "-- \n",
    "\n",
    "We will study both the ground state and the time evolution of a paradigmatic quantum system. \n",
    "\n",
    "Specifically, we will consider the transverse-field Ising (TFI) model on a 2D square lattice. \n",
    "The Hamiltonian is:\n",
    "\n",
    "$$ \n",
    "\\mathcal{H}= - \\Gamma\\sum_{i}\\sigma_{i}^{x} - V\\sum_{\\langle i, j \\rangle}\\sigma_{i}^{z}\\sigma_{j}^{z}, \n",
    "$$\n",
    "where $\\sigma_{i}^{x/z}$ represents the $x$ and $z$ Pauli operators on the lattice site $i$. \n",
    "$\\Gamma$ is the transverse magnetic field and $V$ is the coupling strength.  \n",
    "The sum $\\langle i, j \\rangle$ runs over nearest neighbors and we assume periodic boundary conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bddf19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Installing Netket \n",
    "\n",
    "First of all, you need to install netket. You can do so by running the following cell:\n",
    "\n",
    ":::{note}\n",
    "If you are executing this notebook on **Colab**, you will need to install NetKet. \n",
    "You can do so by uncommenting and running the following cell.\n",
    "\n",
    "Keep in mind that this notebook was designed for NetKet version `3.11.1`, which requires Python >=3.9. If you do not have access to a recent version of Python we strongly recomend to run this notebook on google Colab.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e97750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet netket matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f424667",
   "metadata": {},
   "source": [
    "You can check that the installation was successful doing by importing the library:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7effe",
   "metadata": {},
   "source": [
    "## 1. Defining The Hamiltonian\n",
    "\n",
    "The first step in our journey consists in defining the Hamiltonian we are interested in as a Netket operator. \n",
    "\n",
    "For this purpose, we first need to define the degrees of freedom we are dealing with (i.e. spins, bosons, fermions etc) by specifying the Hilbert space of the problem. \n",
    "\n",
    "For example, let us consider a $4\\times4$ square lattice of spins $1/2$.\n",
    "\n",
    "In Netket, a spin $1/2$ configuration is specified by a string $\\{-1, 1\\}^N$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4  # side of the 2D lattice\n",
    "N = L * L  # number of spins\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N)  # create the Hilbert space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adaa504",
   "metadata": {},
   "source": [
    "Netket is largely based on `jax`, a library for accelerator-oriented array computation and program transformation.\n",
    "It contains a high-performance version of all the functions in in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "print(\n",
    "    \"An Hilbert space configuration looks like: \", hi.random_state(jax.random.key(0), 1)\n",
    ")  # print a random configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b8cfb",
   "metadata": {},
   "source": [
    "To define the Hamiltonian, it is useful to specify an object `nk.graph` which contains all the information on the topology of the physical space where the spin are placed, namely in this case a 2D square lattice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2853d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nk.graph.Square(length=L, pbc=True)  # pbc = periodic boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e67ac",
   "metadata": {},
   "source": [
    "Now, we can create the Hamiltonian by summing $k$-local operators that in Netket are specified as elements of the class ```LocalOperator``` (see details [here](https://netket.readthedocs.io/en/v3.12.1/api/_generated/operator/netket.operator.LocalOperator.html)). \n",
    "\n",
    "In particular, we need only 1-local operators of the type $ \\sigma^{x}_i $ and a 2-local operators of the type $ \\sigma^{z}_i \\sigma^{z}_j $. \n",
    "These are already contained in the class by default and so they can be easily imported. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netket.operator.spin import sigmax, sigmaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41d740",
   "metadata": {},
   "source": [
    "We first consider the interaction 2-local part. \n",
    "We run over all the possible edges of the graph to consider all the possible nearest-neighbor couplings.\n",
    "Note that NetKet automatically recognizes products of local operators as tensor products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 1\n",
    "H = sum([-V * sigmaz(hi, i) * sigmaz(hi, j) for (i, j) in graph.edges()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289bff53",
   "metadata": {},
   "source": [
    "We now add the 1-local terms, considering $\\Gamma = 3.044 \\, V$ which corresponds to the critical point of the phase transition in the 2D TFI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = 3.044 * V\n",
    "H += sum([-Gamma * sigmax(hi, i) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65581d8b",
   "metadata": {},
   "source": [
    "## 2. Ground state\n",
    "The first problem we want to solve is finding the ground state of the above Hamiltonian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823e185",
   "metadata": {},
   "source": [
    "### 2.1 Exact benchmark \n",
    "Since we are playing with a number of spins that is still manageable with exact diagonalization methods, we can compute a benchmark for later. \n",
    "\n",
    "This is easily done in Netket by converting the local operator into a sparse $2^N \\times 2^N$ matrix, which can be then efficiently diagonalized with methods of standard libraries such as `eigsh` in `scipy.sparse.linalg` which employs the Lanczos method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_sp = H.to_sparse()\n",
    "print(\"The Hamiltonian matrix has shape: \", H_sp.shape)\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "eig_vals, eig_vecs = eigsh(\n",
    "    H_sp, k=2, which=\"SA\"\n",
    ")  # k is the number of eigenvalues desired,\n",
    "E_gs = eig_vals[0]  # \"SA\" selects the ones with smallest absolute value\n",
    "\n",
    "print(\"The ground state energy is:\", E_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7fe2a",
   "metadata": {},
   "source": [
    "### 2.2 Variational Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b3a35",
   "metadata": {},
   "source": [
    "Netket finds the ground state energy of the system by running the Variational Monte Carlo (VMC) method.\n",
    "\n",
    "The idea is to choose a variational ansatz that approximates the quantum state of the system $|\\Psi\\rangle \\approx |\\Psi_{\\theta}\\rangle$ and to minimize the energy computed on it with respect to its parameters $\\theta$. \n",
    "Thanks to the Rayleigh-Ritz variational principle, we are guaranteed to converge to a variational approximation of the true ground state since: \n",
    "\\begin{equation}\n",
    "E_{\\theta} = \\frac{\\langle \\Psi_{\\theta} | H | \\Psi_{\\theta}\\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta}\\rangle} \\geq E_{\\mathrm{GS}}\n",
    "\\end{equation}\n",
    "for any choice of $\\theta$. \n",
    "\n",
    "The Monte Carlo enters in evaluating the energy and its gradient, which is needed for the procedure of minimization. \n",
    "\n",
    "The choice of the ansatz is crucial and directly affect the accuracy of the variational approximation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51cc79",
   "metadata": {},
   "source": [
    "#### 2.2.1 Mean-field ansatz\n",
    "\n",
    "We start by considering the simplest ansatz, namely a mean-field state:\n",
    "\n",
    "$$ \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{MF}} \\rangle = \\Pi_{i=1}^{N} \\Phi_i(\\sigma^{z}_i), $$\n",
    "\n",
    "The variational parameters are contained in the single-spin wave functions $\\Phi_i(\\sigma^{z}_i)$. \n",
    "\n",
    "For the single-spin wave functions we take a sigmoid form: \n",
    "\n",
    "$$ \\Phi_i(\\sigma^{z}_i) = 1/(1+\\exp(-\\lambda \\sigma^z_i)), $$\n",
    "\n",
    "thus depending on the complex-valued variational parameter $\\lambda$. \n",
    "$\\lambda$ can be different for different $\\Phi_i$, but here we take it to be the same for simplicity. \n",
    "\n",
    "**IMPORTANT**: in NetKet one has to define a variational function approximating the **logarithm of the wave-function amplitudes**, and not the wave function amplitudes themselves.\n",
    "Moreover, the ansatz is a function evaluated on batches of many-body configurations, namely on a (`jax.numpy`) array of dimension $(N_s, N)$ where $N_s$ is the dimension of the batch (a.k.a. number of samples) and $N$ is the number of local degrees of freedom. \n",
    "\n",
    "The model is defined as a class inheriting from `flax.linen.Module`, where `flax` is a library for models based on `jax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    # The __call__(self, x) function should take as\n",
    "    # input a batch of states x.shape = (n_samples, N)\n",
    "    # and should return a vector of n_samples log-amplitudes\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Extract the system size\n",
    "        N = x.shape[-1]\n",
    "\n",
    "        # A tensor of variational parameters is defined by calling\n",
    "        # the method `self.param` where the arguments will be:\n",
    "        lam = self.param(\"lambda\",                     # arbitrary name used to refer to this set of parameters\n",
    "                         nn.initializers.normal(1.0),  # an initializer used to provide the initial values. 1.0 is the std deviation\n",
    "                         (1,),                         # The shape of the tensor\n",
    "                         complex)                      # The dtype of the tensor.\n",
    "\n",
    "        # compute the probabilities\n",
    "        p = nn.log_sigmoid(lam * x)\n",
    "\n",
    "        # sum the output\n",
    "        out = 0.5 * jnp.sum(p, axis=-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64b716-25ef-4e29-8903-7afe1c1b2738",
   "metadata": {},
   "source": [
    "This defines the variational ansatz, or variational wave-function.\n",
    "\n",
    "To use it, you must do the following things:\n",
    "   1) Create an instance of that class. In this case `MF()`, in other cases you might specify some hyperparameters as well. This corresponds to the parametrised function itself, or the logic, but NOT the parameters.\n",
    "   2) Call the `.init(random_key, sample_input)` method, which takes as input a `jax.random.key()` rng generator to generate the initial random parameters, and  a sample input. The sample input can be anything, it just needs to have the good shape and datatype that is used to infer the shape of all parameters in the network. The output of this function is a Pytree of parameters, aka a dictionary of variables\n",
    "   3) To evaluate the function, you must do `MF().apply(variables, input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63648f15-2e59-48f6-a7e9-b370896433f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "model = MF()\n",
    "\n",
    "# generate a random sample, and create an initialization for the parameters (stored as pytree dictionary)\n",
    "sample_input = hi.random_state(jax.random.key(0), 1)\n",
    "params = model.init(jax.random.key(0), sample_input)\n",
    "\n",
    "# This is how you evaluate the network for those parameters. \n",
    "model.apply(params, sample_input)\n",
    "\n",
    "# It should work also with many input samples\n",
    "sample_input = hi.random_state(jax.random.key(0), 10)\n",
    "model.apply(params, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccb969-487e-43e6-ac87-50cde68c68e1",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "In Netket we operates with Monte Carlo variational states, that are objects from which we can sample from and for which we can easy compute expectation values of operators and correspoding gradients. \n",
    "Therefore, besides the wave function, we need to construct a sampler.\n",
    "While you could write your own MCMC sampling code, writing an efficient one and making sure it can scale to multiple GPUs is not straightforward (though not hard even). \n",
    "For that reason, we will be using the samplers from NetKet, of which there are several and can be found in `netket.sampler.*`, and are all instances instance of the class `nk.sampler.Sampler`.\n",
    "\n",
    "In Netket, the samples are drawn in parallel from many chains, and so they are collected in an array of dimension `(n_chains, chain_length, N)`, such that `n_samples = n_chains * chain_length`. \n",
    "\n",
    "We will be using a simple sampler based on the Metropolis-Hastings algorithm, with a local transition rule that consists in flipping a single spin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb0ac3-bc4c-4743-adec-03bb4e2d7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the local Metropolis sampler on the Hilbert space\n",
    "sampler = nk.sampler.MetropolisLocal(\n",
    "    hi,\n",
    "    n_chains=16,        # we specify 16 chains\n",
    "    sweep_size=hi.size, # every sample is obtained by sweeping the sampler for hi.size=16 times, meaning that we are\n",
    "                        # effectively generating a chain of length chain_length*sweep_size , and taking only 1 ever sweep size samples.\n",
    "                        # this is done to reduce correlations between two samples.\n",
    ")\n",
    "\n",
    "# create the state of the sampler\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "sampler_state = sampler.reset(model, params, sampler_state)\n",
    "\n",
    "# sample the configurations. \n",
    "Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the samples is: \", Ïƒ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2053a-6444-4363-b5cf-a7d624c5543a",
   "metadata": {},
   "source": [
    "And you can see that the samples will be returned as a tensor with 3 indices, where the first axis corresponds to the different chains, the second axis corresponds to the length of each chain, and the third axis corresponds to the different degrees of freedom of your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528fe6e-2dcd-40b2-8521-163f43e478f6",
   "metadata": {},
   "source": [
    "You can call `sampler.sample`as many time as you want in a training loop. \n",
    "However, every time you change the parameters you should call `sampler.reset`.\n",
    "Depending on the sampler that you are using, this may do something (like reset some intenral caches) or nothing at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4c0e4-bf39-476d-8f8f-9fe966069e7d",
   "metadata": {},
   "source": [
    "### Expectation Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f23399-7a3c-43c1-8d1c-14f8d5f13ea3",
   "metadata": {},
   "source": [
    "In Netket, you can compute expectation values of operators and the corresponding gradient using built-in functions. \n",
    "However, for educational purposes, we will write it from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07dd0b",
   "metadata": {},
   "source": [
    "In VMC, we compute the expectation value of an operator as a statistical average of an estimator over samples drawn from the Born probability distribution of the quantum state. \n",
    "\n",
    "For the energy, we can write:\n",
    "$$\n",
    "   E_{\\theta} = \\frac{\\langle \\Psi_{\\theta} | H | \\Psi_{\\theta} \\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle}  = \\sum_\\sigma \\frac{|\\Psi_{\\theta}(\\sigma)|^2}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle} \\frac{\\langle \\sigma | H | \\Psi_{\\theta} \\rangle}{\\langle \\sigma | \\Psi_{\\theta} \\rangle} = \\mathbb{E}_{|\\Psi_{\\theta}(\\sigma)|^2}[E_{\\mathrm{loc}}(\\sigma)]\n",
    "$$\n",
    "where $E_{\\mathrm{loc}}(\\sigma) = \\langle \\sigma | H | \\Psi_{\\theta} \\rangle / \\langle \\sigma | \\Psi_{\\theta} \\rangle$. \n",
    "\n",
    "The statistical mean is approximated as the empirical mean over a set of samples $\\sigma^{(i)}$ drawn from $|\\Psi_{\\theta}(\\sigma)|^2 / \\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle$ as: \n",
    "$$\n",
    "E_{\\theta} \\approx  \\frac{1}{N_s} \\sum_{i=1}^{N_s} E_{\\mathrm{loc}}(\\sigma^{(i)}).\n",
    "$$\n",
    "\n",
    "For the gradient, we have a similar formula as a statistical average: \n",
    "$$\n",
    "    \\partial_k E_{\\theta} = \\mathbb{E}_{|\\Psi_{\\theta}(\\sigma)|^2} \\left[ (\\partial_k \\log\\Psi_{\\theta}(\\sigma))^* \\left( E_\\text{loc}(\\sigma) - \\langle E \\rangle\\right)\\right] \\approx \\frac{1}{N_s}\\sum_i^{N_s} (\\partial_k \\log\\Psi_{\\theta}(\\sigma_i))^* \\left( E_\\text{loc}(\\sigma_i) - \\langle E \\rangle\\right)\n",
    "$$ \n",
    "\n",
    "To compute both the energy and the gradient, we need a function computing $E_{\\mathrm{loc}}(\\sigma)$ and $\\partial_k \\log \\Psi_{\\theta}(\\sigma)$ for some sample batch $\\sigma$. In all the following we will consider holomorphic wave functions. \n",
    "\n",
    "For the local energies: \n",
    "$$\n",
    "E_{\\mathrm{loc}}(\\sigma) = \\frac{\\langle \\sigma | H | \\Psi_{\\theta} \\rangle}{\\langle \\sigma | \\Psi_{\\theta} \\rangle} = \\sum_{\\eta}  H_{\\sigma\\eta} \\frac{\\Psi_{\\theta}(\\eta)}{\\Psi_{\\theta}(\\sigma)}\n",
    "$$\n",
    "we can use the function `nk.get_conn_padded` which, given $\\sigma$, computes the connected configurations $|\\eta\\rangle$ and the matrix elements $H_{\\sigma\\eta}$ such that $H_{\\sigma\\eta} \\neq 0$.\n",
    "\n",
    "$\\partial_k \\log \\Psi_{\\theta}(\\sigma)$ is the jacobian of the function of $\\log \\Psi_{\\theta}(\\sigma): \\mathbb{C}^{N_{\\mathrm{pars}}} \\rightarrow \\mathbb{C}^{N_{\\mathrm{samples}}}$. \n",
    "This can be computed using the function `jax.jacrev`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïƒ = Ïƒ.reshape(-1, N) # we need to reshape it to (n_chains * chain_lenght, N) = (n_samples, N)\n",
    "eta, eta_H_sigma = H.get_conn_padded(Ïƒ)\n",
    "\n",
    "print(\"The shape of the samples is: \", Ïƒ.shape) # (n_samples, N)\n",
    "print(\"The shape of the connected configurations is: \", eta.shape)  # (n_samples, n_conn, N)\n",
    "print(\"The shape of the matrix elements is: \", eta_H_sigma.shape)  # (n_samples, n_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpsi_fun = lambda pars : model.apply(pars, Ïƒ) # we freeze the samples so the functions depend only on the parameters\n",
    "jacobian = jax.jacrev(logpsi_fun, holomorphic=True)(params)   # compute the jacobian of the log of the (holomorphic) wavefunction\n",
    "print(\"The parameters of jastrow have shape:\\n\" , jax.tree.map(lambda x: x.shape, params))  # (N_par, )\n",
    "print(\"The jacobian of jastrow have shape:\\n\" , jax.tree.map(lambda x: x.shape, jacobian))  # (n_samples, N_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ad9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    " \n",
    "# TODO\n",
    "\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def compute_eloc(model, parameters, ham, Ïƒ):\n",
    "    # reshape the samples to have shape (n_samples, N), the samples are divided in different Markov chains\n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute the connected configurations and the matrix elements\n",
    "    eta, eta_H_sigma = ...\n",
    "\n",
    "    # compute the local energies (in log-spacde for numerical stability)\n",
    "    logpsi_eta = ...    # evaluate the wf on the samples\n",
    "    logpsi_sigma = ...  # evaluate the wf on the connected configurations\n",
    "    logpsi_sigma = ...  # add a dimension to match the shape of logpsi_eta\n",
    "    E_loc = ...         # compute the local energies\n",
    "\n",
    "    return E_loc\n",
    "\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def compute_jacobian(model, parameters, ham, Ïƒ):\n",
    "    # reshape the samples, the samples are divided in different Markov chains\n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute the dk logpsi\n",
    "    logpsi_fun = ...    # we freeze the samples so the functions depend only on the parameters\n",
    "    jacobian = ...  # compute the jacobian of the log of the (holomorphic) wavefunction\n",
    "\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def estimate_energy_and_gradient(model, parameters, ham, Ïƒ):\n",
    "    # reshape the samples, the samples are divided in different Markov chains\n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute eloc\n",
    "    E_loc = ...\n",
    "\n",
    "    # compute jacobian\n",
    "    jacobian = ...\n",
    "\n",
    "    # take the number of samples\n",
    "    n_samples = E_loc.shape[0]\n",
    "\n",
    "    # compute the energy\n",
    "    E_average = ...\n",
    "    E_variance = ...\n",
    "    E_error = ...\n",
    "    E = nk.stats.Stats(mean=E_average, error_of_mean=E_error, variance=E_variance)  # create a Netket object containing the statistics\n",
    "\n",
    "    # center the local energies\n",
    "    E_loc -= E_average\n",
    "\n",
    "    # compute the gradient as Ok.conj() @ E_loc / n_samples (operate on pytree with jax.tree.map) \n",
    "    E_grad = ...\n",
    "\n",
    "    return E, E_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046d9c2",
   "metadata": {},
   "source": [
    "To work with jax, we make the hamiltonian compatible with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b206ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_jax = H.to_pauli_strings().to_jax_operator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, E_grad = estimate_energy_and_gradient(model, params, H_jax, Ïƒ)\n",
    "\n",
    "print(\"The energy is: \", E)\n",
    "print(\"The energy gradient is: \", E_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b6302",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2.2 VMC from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab77267-892e-48d2-97e2-eee6c8eae796",
   "metadata": {},
   "source": [
    "We will now optimize the parameters of the ansatz in order to best approximate the ground state of the Hamiltonian.\n",
    "\n",
    "For educational purposes, we will write the VMC loop from scratch. \n",
    "\n",
    "To minimize the energy we can use the standard Stochastic Gradient Descent (SGD) scheme, which updates the parameters according to: \n",
    "\\begin{equation}\n",
    "\\theta^{\\mathrm{new}} = \\theta^{\\mathrm{old}} - \\eta \\nabla E(\\theta^\\mathrm{old}), \n",
    "\\end{equation}\n",
    "where $\\eta$ defines the step size and is called learning rate in the Machine Learning community. \n",
    "\n",
    "If we repeat this scheme for many iterations we are guaranteed to converge to a local minimum of the energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b0687-1f78-4c2e-9f17-16c3fac2f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "from tqdm import tqdm  # tqdm is just a progress bar\n",
    "\n",
    "energies_mf = []\n",
    "n_steps = ...\n",
    "eta = ...\n",
    "\n",
    "# initialize the parameters and the sampler\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad = ...\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_mf.append(E.mean.real)\n",
    "    \n",
    "    # performs the SGD update function on every element of the dictionary containing the set of parameters\n",
    "    params = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22ec7f-4479-47d4-a921-b568a4280c4f",
   "metadata": {},
   "source": [
    "We can now plot the energy during those optimization steps and compare it with the benchmark previously calculated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f2a0e-3408-443d-8f9b-13f70db15231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(energies_mf, label=\"MF\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.hlines(E_gs, 0, len(energies_mf), linestyles=\"--\", color=\"black\", label=\"ED\")\n",
    "plt.legend()\n",
    "\n",
    "print(\n",
    "    \"The relative error with the benchmark is: \",\n",
    "    jnp.absolute((energies_mf[-1] - E_gs) / E_gs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0e656",
   "metadata": {},
   "source": [
    "#### 2.2.4. Jastrow ansatz\n",
    "\n",
    "We can now use a more correlated ansatz than the simple mean-field. \n",
    "\n",
    "For instance, we can take a 2-body Jastrow ansatz entangling different spins, namely:\n",
    "\n",
    "$$ \\log \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{Jas}} \\rangle =  \\sum_{i, j} J_{ij} \\sigma^{z}_i\\sigma^{z}_{j} $$\n",
    "\n",
    "where the variational parameter is $J$. \n",
    "\n",
    "Again we can write the model using `flax`.  If you want to look at the official documentation, look [here](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#defining-your-own-models).\n",
    "In general, you must write a function that computes the formula above for an arbitrary input $x$.\n",
    "\n",
    "In general, it is possible to implement the formula above in two ways:\n",
    " - Either we do an einstein-summation over i and j above, but the $J$ matrix must be simmetric\n",
    " - Or we just do a summation over $i<j$.\n",
    "\n",
    "The second idea, however, while good for C++, will be very inefficient in Jax which requires vectorised operations. So the 'only' good way to do this is by constructing a J matrix that is simmetric. \n",
    "In general, if you have a matrix J of NxN parameters, the stochastic optimisation might break its hermitianity, so you need to find a way to ensure that it's hermitian.\n",
    "Either you paramtrise only the $N*(N-1)/2$ actually free parameters and you construct J from that, or you parametrise a $NxN$ matrix but you use a symmetric J. Pick one! (The latter is easier than the former approach).\n",
    "\n",
    "Note: To make your network work with inputs that have an arbitrary number of batch dimensions, like a single bitstring, or a vector of bistrings... you can replace the matrix multiplication $J@ x$ with an einstein summation `jnp.einsum('ij,...j', J, x)`  which is the same thing, but will carry over the extra dimensions in $x$ labelled by `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73212379-b39f-4e69-966a-1a4b990dd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class Jastrow(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # take the system size \n",
    "        N = x.shape[-1]\n",
    "\n",
    "        # Define the variational parameter \n",
    "        Params = ...\n",
    "\n",
    "        # Create the symmetric J matrix from the parameters\n",
    "        J = ...\n",
    "\n",
    "        # x.T@(J@x) but with einsum\n",
    "        y = ... #jnp.einsum(...)\n",
    "\n",
    "        return y\n",
    "    \n",
    "model = Jastrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951ecfb-533f-4a92-bed5-0caa6402a5b0",
   "metadata": {},
   "source": [
    "Now try to run again the optimisation using this ansatz, and see if it improves on the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb96859-d3bf-427c-9a4e-da2fe679e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "from tqdm import tqdm  # tqdm is just a progress bar\n",
    "\n",
    "energies_jastrow = []\n",
    "\n",
    "# copy from above...\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # sample\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad = ...\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_jastrow.append(E.mean.real)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a75538-6d88-4fea-8e32-b1c6f9c237d9",
   "metadata": {},
   "source": [
    "And let's plot the eenergies again and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25bdb0-2d5c-4141-a914-4721553ef7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(energies_mf, label=\"MF\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.hlines(E_gs, 0, len(energies_mf), linestyles=\"--\", color=\"black\", label=\"ED\")\n",
    "plt.legend()\n",
    "\n",
    "print(\n",
    "    \"The relative error with the benchmark is: \",\n",
    "    jnp.absolute((energies_mf[-1] - E_gs) / E_gs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0ec32-0d68-4f93-b382-8581b674bb93",
   "metadata": {},
   "source": [
    "## Using different optimisers: Adam & Company\n",
    "\n",
    "Until now you used a very 'simple' (stochastic) gradient descent, but there are much more advanced optimisation schemes available nowdays that rely on the gradient.\n",
    "For example, we could use adaptive momentum.\n",
    "\n",
    "To do that, we can rely on a package called [optax](https://optax.readthedocs.io) which implements several optimisers itself. \n",
    "Below, I show you a simple example of how to implement the training loop using optax. \n",
    "You should use this as a playground to try using [ADAM](https://arxiv.org/abs/1412.6980) to optimise the Jastrow (note: for jastrows it will not improve much, but for later networks, it will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc90ca4-c5ab-4f68-b7b0-097a4ea74a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "import optax\n",
    "\n",
    "from tqdm import tqdm  # tqdm is just a progress bar\n",
    "\n",
    "energies_jastrow_adam = []\n",
    "n_steps = ...\n",
    "eta = ...\n",
    "\n",
    "\n",
    "# initialize the parameters and the sampler\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "optimizer_state = optimizer.init(params)\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad = ...\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_jastrow_adam.append(E.mean.real)\n",
    "    \n",
    "    # performs the optax update\n",
    "    updates, optimizer_state = optimizer.update(E_grad, optimizer_state)\n",
    "    params = optax.apply_updates(params, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49722d29-d59e-4fed-80c6-ac75e7766952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the energies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c960b-f257-4d3b-bd76-40d68ed33392",
   "metadata": {},
   "source": [
    "###  Neural Quantum State (NQS) : A 1 layer Perceptron (Aka, Carleo's RBM) \n",
    "\n",
    "We now want to use a more sophisticated ansatz, based on a neural network representation of the wave function. \n",
    "\n",
    "We consider a two-layer fully-connected feed-forward neural network, whose mathematical expression is:\n",
    "\n",
    "$$ \\log \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{NQS}} \\rangle =  g(W_2 \\cdot g (W_1 \\cdot \\sigma^z + b_1) + b_2)$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{C}^{N \\times M}, b_1 \\in \\mathbb{C}^M, W_2 \\in \\mathbb{C}^{M \\times 1}$ and $b_2 \\in \\mathbb{C}$ are the variational parameters, and $g$ is a non-linear function acting element-wise. \n",
    "$W_1$ and $b_1$ belongs to the first dense layer, while $W_2$ and $b_2$ belongs to the second.\n",
    "$M$ is the hidden-unit density and parametrizes the expressibility of the ansatz. \n",
    "Here we choose $g$ to be the logcosh activation function. \n",
    "\n",
    "Use the functions in `flax.linen` and the activation function `nk.nn.activation.log_cosh`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcc2bb-06da-4e2f-8061-632baa14aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "class NQS(nn.Module):\n",
    "    M: int = 1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        dense1 = ...\n",
    "        dense2 = ...\n",
    "\n",
    "        # We apply the dense layer to the input\n",
    "        y = dense1(x)\n",
    "\n",
    "        # We apply the activation function\n",
    "        y = ...\n",
    "\n",
    "        # We apply the final dense layer\n",
    "        y = dense2(y)[..., 0]\n",
    "\n",
    "        # We apply the activation function\n",
    "        out = ...\n",
    "\n",
    "        # sum the output\n",
    "        return out \n",
    "\n",
    "model = NQS(M = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a06a18-46b3-438d-b02f-b79c929044f6",
   "metadata": {},
   "source": [
    "And now try to optimise it with SGD and Adam (you can use the same training loop based on optax, and running it with `optax.sgd(learning_rate=0.01)` and `optax.adam(learning_Rate=0.01)` and compare it both with the jastrow, and among them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb00f0-342b-4fbf-a7a4-2b1b58b56006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2cc05-8582-4dc9-a2af-54f8afbd82d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af83bed-0f7b-4546-ae8f-7fa38c936e47",
   "metadata": {},
   "source": [
    "## Advanced optimisation: Stochastic Reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616acc8",
   "metadata": {},
   "source": [
    "We then optimize it, however this time adding the Stochastic Reconfiguration (SR) preconditioner (also known as Quantum Natural Gradient). \n",
    "SR preconditiones the gradient by the substitution: \n",
    "$$\n",
    "\\nabla E(\\theta) \\longrightarrow S^{-1} \\nabla E(\\theta)\n",
    "$$\n",
    "in the SGD update scheme. \n",
    "\n",
    "$S$ is the so-called Quantum Geometric Tensor (QGT), and it corresponds to the metric tensor of the Fubini-Study metrics in the Hilbert space. \n",
    "The Fubini-Study metrics is the natural metrics among quantum states and it is defined as: \n",
    "$$\n",
    "d_{\\mathrm{FS}}(|\\psi\\rangle, |\\phi\\rangle) = \\arccos\\bigg(\\frac{\\langle \\phi | \\psi \\rangle \\langle \\psi | \\phi \\rangle}{\\langle \\phi | \\phi \\rangle \\psi \\langle \\psi \\rangle}\\bigg).\n",
    "$$\n",
    "\n",
    "Indeed, for an infinitesimal parameter change we have that: \n",
    "$$\n",
    "d_{\\mathrm{FS}}(|\\Psi_{\\theta}\\rangle, |\\Psi_{\\theta + \\delta \\theta}\\rangle) =  \\arccos\\bigg(\\frac{\\langle \\Psi_{\\theta + \\delta \\theta} | \\Psi_{\\theta} \\rangle \\langle \\Psi_{\\theta} | \\Psi_{\\theta + \\delta \\theta} \\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta}  \\rangle  \\langle  \\Psi_{\\theta + \\delta \\theta} | \\Psi_{\\theta + \\delta \\theta} \\rangle}\\bigg) \\approx \\delta \\theta^\\dagger S \\delta \\theta.\n",
    "$$\n",
    "\n",
    "The QGT is defined as: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "S_{ij} &= \\frac{\\langle \\partial_i \\Psi_{\\theta} | \\partial_j \\Psi_{\\theta} \\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle} - \\frac{\\langle \\partial_i \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle} \\frac{\\langle \\Psi_{\\theta} | \\partial_j \\Psi_{\\theta} \\rangle}{\\langle \\Psi_{\\theta} | \\Psi_{\\theta} \\rangle} = \\\\ \n",
    "&= \\sum_\\sigma \\frac{|\\Psi_{\\theta}(\\sigma )|^2}{\\langle\\Psi_{\\theta}|\\Psi_{\\theta}\\rangle} (\\partial_i \\log\\Psi_{\\theta}(\\sigma ) - \\langle\\partial_i \\log\\Psi_{\\theta} \\rangle)^* (\\partial_j \\log\\Psi_{\\theta}(\\sigma) - \\langle\\partial_j \\log\\Psi_{\\theta} \\rangle) =  \\\\\n",
    "&= \\frac{1}{N_s} \\sum_{m=1}^{N_s} \\bigg(\\partial_i \\log\\Psi_{\\theta}(\\sigma^{(m)}) - \\frac{1}{N_s} \\sum_{k=1}^{N_s} \\partial_i \\log\\Psi_{\\theta}(\\sigma^{(k)})\\bigg)^* \\bigg(\\partial_j \\log\\Psi_{\\theta}(\\sigma^{(m)}) - \\frac{1}{N_s} \\sum_{l=1}^{N_s} \\partial_j \\log\\Psi_{\\theta}(\\sigma^{(l)})\\bigg)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We need a function that computes the QGT. We can reuse $\\log\\Psi_{\\theta}(\\sigma )$ from the computation of the gradient of the energy.\n",
    "So we redefine the function to return also the jacobian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def estimate_energy_and_gradient_and_jacobian(model, parameters, ham, Ïƒ):\n",
    "    # reshape the samples, the samples are divided in different Markov chains\n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute eloc\n",
    "    E_loc = ...\n",
    "\n",
    "    # compute jacobian\n",
    "    jacobian = ...\n",
    "\n",
    "    # take the number of samples\n",
    "    n_samples = E_loc.shape[0]\n",
    "\n",
    "    # compute the energy\n",
    "    E_average = ...\n",
    "    E_variance = ...\n",
    "    E_error = ...\n",
    "    E = nk.stats.Stats(mean=E_average, error_of_mean=E_error, variance=E_variance)  # create a Netket object containing the statistics\n",
    "\n",
    "    # center the local energies\n",
    "    E_loc -= E_average\n",
    "\n",
    "    # compute the gradient as Ok.conj() @ E_loc / n_samples (operate on pytree with jax.tree.map) \n",
    "    E_grad = ...\n",
    "\n",
    "    return E, E_grad, jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587f235",
   "metadata": {},
   "source": [
    "For practical purposes, it is now more easy to work with dense representations of the gradients and of the jacobian: to convert them from `pytree` to `jnp.array` we can use the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_jacobian(pytree):\n",
    "    # Apply reshape operation to each leaf of the pytree\n",
    "    reshaped_pytree = jax.tree.map(lambda x: jnp.reshape(x, (x.shape[0], -1)), pytree)\n",
    "    \n",
    "    # Convert the pytree to a flat list of arrays\n",
    "    flat_list, _ = jax.tree_util.tree_flatten(reshaped_pytree)\n",
    "    \n",
    "    # Stack the arrays in the list along the second dimension\n",
    "    result = jnp.concatenate(flat_list, axis=-1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "sampler_state = sampler.reset(model, params, sampler_state)\n",
    "Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)\n",
    "E, E_grad, jacobian = estimate_energy_and_gradient_and_jacobian(model, params, H_jax, Ïƒ)\n",
    "\n",
    "print(\"The flatten jacobian has shape: \", flatten_jacobian(jacobian).shape)\n",
    "print(\"The flatten grad has shape: \", nk.jax.tree_ravel(E_grad)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58305694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "def SR(E_grad, jacobian):\n",
    "\n",
    "    # convert from pytree to dense array\n",
    "    E_grad_dense, unravel = nk.jax.tree_ravel(E_grad)\n",
    "    jacobian_dense = flatten_jacobian(jacobian)\n",
    "\n",
    "    # take the number of samples\n",
    "    n_samples = jacobian_dense.shape[0]\n",
    "\n",
    "    # center the jacobians\n",
    "    jacobian_centered = ...\n",
    "\n",
    "    # compute the S matrix\n",
    "    S = ...\n",
    "\n",
    "    # condition the S matrix \n",
    "    S = S + 1e-4 * jnp.eye(S.shape[0])\n",
    "    \n",
    "    # solve the linear system (use the system jax.scipy.sparse.linalg.cg)\n",
    "    E_grad_nat = ...\n",
    "\n",
    "    return unravel(E_grad_nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ae9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "energies_jastrow = []\n",
    "n_steps = ...\n",
    "eta = ...\n",
    "\n",
    "# initialize the parameters\n",
    "model = Jastrow()\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# Initialize the sample\n",
    "sampler = nk.sampler.MetropolisLocal(\n",
    "    hi, \n",
    "    n_chains=16,    # we specify 16 chains\n",
    ")\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad, jacobian = ...\n",
    "\n",
    "    # so SR \n",
    "    E_grad_nat = ...\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_jastrow.append(E.mean.real)\n",
    "\n",
    "    # performs the SGD update function on every element of the dictionary containing the set of parameters\n",
    "    params = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies_mf, label=\"MF\")\n",
    "plt.plot(energies_jastrow, label=\"Jastrow (SR)\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.hlines(E_gs, 0, len(energies_jastrow), linestyles=\"--\", color=\"black\", label=\"ED\")\n",
    "plt.legend()\n",
    "\n",
    "print(\n",
    "    \"The relative error of mf is: \",\n",
    "    jnp.absolute((energies_mf[-1] - E_gs) / E_gs),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The relative error of Jastrow is: \",\n",
    "    jnp.absolute((energies_jastrow[-1] - E_gs) / E_gs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41529d-458e-49d4-bdf8-1345216f6cf7",
   "metadata": {},
   "source": [
    "###  NQS with SR\n",
    "\n",
    "Now let's try to compare how well training compares with and without SR, when training a NQS such as Carleo's RBM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NQS(M = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29016f4",
   "metadata": {},
   "source": [
    "We then proceed to the optimization as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_nqs = []\n",
    "n_steps = 200\n",
    "eta = 0.01\n",
    "\n",
    "# initialize the parameters\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "\n",
    "# Initialize the sample\n",
    "sampler = nk.sampler.MetropolisLocal(\n",
    "    hi, \n",
    "    n_chains=16,    # we specify 16 chains\n",
    ")\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=100)\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad, jacobian = estimate_energy_and_gradient_and_jacobian(model, params, H_jax, Ïƒ)\n",
    "\n",
    "    # so SR \n",
    "    E_grad_nat = SR(E_grad, jacobian)\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_nqs.append(E.mean.real)\n",
    "\n",
    "    # performs the SGD update function on every element of the dictionary containing the set of parameters\n",
    "    params = jax.tree.map(lambda x, y: x - eta * y, params, E_grad_nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10580934-c128-47ef-beb4-315420d63b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies_mf, label=\"MF\")\n",
    "plt.plot(energies_jastrow, label=\"Jastrow (SR)\")\n",
    "plt.plot(energies_nqs, label=\"NQS (SR)\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.hlines(E_gs, 0, len(energies_nqs), linestyles=\"--\", color=\"black\", label=\"ED\")\n",
    "plt.legend()\n",
    "\n",
    "print(\n",
    "    \"The relative error of mf is: \",\n",
    "    jnp.absolute((energies_mf[-1] - E_gs) / E_gs),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The relative error of Jastrow is: \",\n",
    "    jnp.absolute((energies_jastrow[-1] - E_gs) / E_gs),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The relative error of NQS is: \",\n",
    "    jnp.absolute((energies_nqs[-1] - E_gs) / E_gs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20440372",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energies_mf, label=\"MF\")\n",
    "plt.plot(energies_jastrow, label=\"Jastrow (SR)\")\n",
    "plt.plot(energies_nqs, label=\"NQS (SR)\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylim(E_gs - 0.1, E_gs + 0.1)\n",
    "plt.hlines(E_gs, 0, len(energies_nqs), linestyles=\"--\", color=\"black\", label=\"ED\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc1526",
   "metadata": {},
   "source": [
    "### **NOTE**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72ffbf",
   "metadata": {},
   "source": [
    "You can speed up the calculation of the gradient (and of the $S$ matrix) by implementing directly the vector-Jacobian product (in acronym `vjp`) instead of building the Jacobian first and then apply it to the vector of the values of $E_{\\mathrm{loc}}$. \n",
    "Indeed, what `jacrev` does for building the full Jacobian matrix is vectorizing the `vjp` for each parameters.\n",
    "In for the `vjp` you can use the function `nk.jax.vjp` (refinement of `jax.vjp`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7891c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# compute the E_loc and center it outside\n",
    "E_loc = compute_eloc(model, params, H_jax, Ïƒ)\n",
    "E_loc -= jnp.mean(E_loc)\n",
    "\n",
    "# compute the jacobian and apply it\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def build_jacobian_and_apply(model, parameters, Ïƒ, E_loc):\n",
    "    #reshape the samples \n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute the dk logpsi\n",
    "    logpsi_fun = lambda pars : model.apply(pars, Ïƒ).conj()            # we freeze the samples so the functions depend only on the parameters\n",
    "    jacobian = jax.jacrev(logpsi_fun, holomorphic=True)(parameters)   # compute the jacobian of the log of the (holomorphic) wavefunction\n",
    "    n_samples = E_loc\n",
    "\n",
    "    # apply the jacobian to the local energies\n",
    "    E_grad = jax.tree.map(lambda x: jnp.einsum(\"i..., i -> ...\", x.conj(), E_loc) / n_samples, jacobian)\n",
    "\n",
    "    return E_grad\n",
    "\n",
    "# vector-Jacobian product\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def do_vjp(model, parameters, Ïƒ, E_loc):\n",
    "    #reshape the samples \n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    # compute the vjp fun\n",
    "    logpsi_fun = lambda pars : model.apply(pars, Ïƒ).conj()            # we freeze the samples so the functions depend only on the parameters\n",
    "    _, vjp_fun = nk.jax.vjp(logpsi_fun, parameters, conjugate=True)\n",
    "    n_samples = E_loc\n",
    "\n",
    "    # do the vector jacobian product\n",
    "    E_grad = vjp_fun(E_loc)\n",
    "    E_grad = jax.tree.map(lambda x: x / n_samples, E_grad)\n",
    "\n",
    "    return E_grad\n",
    "\n",
    "print(\"Build the Jacobian and apply : \")\n",
    "%timeit build_jacobian_and_apply(model, params, Ïƒ, E_loc)\n",
    "\n",
    "print(\"vjp : \")\n",
    "%timeit do_vjp(model, params, Ïƒ, E_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f225105",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dacb9",
   "metadata": {},
   "source": [
    "# 3. Time evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab7b6e",
   "metadata": {},
   "source": [
    "The second problem we want to solve is to simulate the unitary dynamics generated by the above Hamiltonian but in 1D for simplicity.\n",
    "\n",
    "In particular, we consider a quantum quench to the critical point from the paramagnetic phase, namely we prepare the initial state to be the ground state of the TFI Hamiltonian with $V = 0$, and then we switch-on the interaction by quenching with $\\{\\Gamma = 1, V = 0.5\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558c1e9",
   "metadata": {},
   "source": [
    "### 3.1 Exact benchmark \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bbc83",
   "metadata": {},
   "source": [
    "We can compute the benchmark using a second-order Taylor scheme for the unitary propagator, that is: \n",
    "\\begin{equation}\n",
    "U(\\delta t) = e^{-i \\delta t H} = 1 - i \\delta t H - \\frac{dt^2}{2} H^2 + O(\\delta t^3)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b12c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 12  # side of the 2D lattice\n",
    "N = L   # number of spins\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N)  # create the Hilbert space\n",
    "graph = nk.graph.Chain(length=L, pbc=True)  # pbc = periodic boundary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_dynamics(state, H_sp, ts, obs_sp):\n",
    "    obs_vals = []\n",
    "    dt = ts[1] - ts[0]\n",
    "\n",
    "    for t in tqdm(ts): \n",
    "        state /= jnp.linalg.norm(state)\n",
    "        obs_vals.append((state.conj() @ (obs_sp @ state)))\n",
    "\n",
    "        Hstate = H_sp @ state\n",
    "        \n",
    "        state = state - 1j * dt * Hstate + 0.5 * (dt**2) * (H_sp @ Hstate)\n",
    "\n",
    "    return state, obs_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49e6c7",
   "metadata": {},
   "source": [
    "During the time evolution, we monitor the average $x$-magnetization $\\langle \\sigma_x \\rangle = \\frac{1}{N} \\sum_i \\sigma_x^i$ as observable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce577e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = sum([sigmax(hi, i) for i in range(N)]) / N\n",
    "obs_jax = obs.to_pauli_strings().to_jax_operator()\n",
    "obs_sp = obs.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare the initial state as the ground state of H with V = 0\n",
    "Gamma = 1.\n",
    "H0 = sum([- Gamma * sigmax(hi, i) for i in range(N)])\n",
    "H0_jax = H0.to_pauli_strings().to_jax_operator()\n",
    "H0_sp = H0.to_sparse()\n",
    "eig_vals, eig_vecs = eigsh(\n",
    "    H0_sp, k=2, which=\"SA\"\n",
    ")\n",
    "GS = eig_vecs[:, 0]\n",
    "E_gs = eig_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c982b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the Hamiltonian for the quench with {Gamma = 6.088, V = 1} and the time parameters\n",
    "V = 0.5\n",
    "H = sum([- Gamma * sigmax(hi, i) for i in range(N)]) + sum([- V * sigmaz(hi, i) * sigmaz(hi, j) for (i, j) in graph.edges()])\n",
    "H_sp = H.to_sparse()\n",
    "H_jax = H.to_pauli_strings().to_jax_operator()\n",
    "ts = jnp.linspace(0, 1.0, 1001)\n",
    "dt = ts[1] - ts[0]\n",
    "print(\"dt = \", dt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27568fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, obs_exact = exact_dynamics(GS, H_sp, ts, obs_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc6fda",
   "metadata": {},
   "source": [
    "### 3.2 time-dependent Variational Monte Carlo (t-VMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4166d1",
   "metadata": {},
   "source": [
    "Netket simulates the dynamics of a quantum system by running the time-dependent Variational Monte Carlo (t-VMC) algorithm.\n",
    "\n",
    "t-VMC projects the time-dependent SchrÃ¶dinger's equation into the variational manifold of the ansatz by exploiting variational principle for dynamics (McLachlan's, Dirac-Frenkel's or the Time-Dependent Variational Principle). \n",
    "\n",
    "This leads to a system of ordinary differential equations for the variational parameters whose solution describes the unitary dynamics on the manifold: \n",
    "\\begin{equation}\n",
    "S(\\theta_t) \\dot{\\theta}_t = F(\\theta_t), \n",
    "\\end{equation}\n",
    "where $S(\\theta_t)$ is the Quantum Geometric Tensor (as in SR) and $F(\\theta_t)$ are the forces defined as $F(\\theta_t) = -i \\nabla E(\\theta_t)$. \n",
    "\n",
    "Once the system is solved for $\\dot{\\theta}_t$ at time $t$, then the solution is propagated with any numerical integration scheme (Euler, Heun, Runge-Kutta, ...) from $\\theta_t$ to $\\theta_{t+\\delta t}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25abc8b",
   "metadata": {},
   "source": [
    "We first prepare the ground state of the Hamiltonian for $V=0$. Take the RBM ansatz (example of one hidden layer NN) from the standard models in Netket `nk.models.RBM` with hidden density `alpha = 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac945a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the model\n",
    "model = nk.models.RBM(alpha=1, param_dtype=complex)\n",
    "\n",
    "energies_nqs = []\n",
    "n_steps = 200\n",
    "eta = 0.01\n",
    "\n",
    "# initialize the parameters\n",
    "params = model.init(jax.random.key(0), hi.random_state(jax.random.key(0), 1))\n",
    "\n",
    "# Initialize the sample\n",
    "sampler = nk.sampler.MetropolisLocal(\n",
    "    hi, \n",
    "    n_chains=16,    # we specify 16 chains\n",
    ")\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# For every iteration\n",
    "for i in tqdm(range(n_steps)):\n",
    "\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=50)\n",
    "\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad, jacobian = estimate_energy_and_gradient_and_jacobian(model, params, H0_jax, Ïƒ)\n",
    "\n",
    "    # so SR \n",
    "    E_grad_nat = SR(E_grad, jacobian)\n",
    "\n",
    "    # log the energy to a list\n",
    "    energies_nqs.append(E.mean.real)\n",
    "\n",
    "    # performs the SGD update function on every element of the dictionary containing the set of parameters\n",
    "    params = jax.tree.map(lambda x, y: x - eta * y, params, E_grad_nat)\n",
    "\n",
    "params_v0 = params\n",
    "\n",
    "print(\n",
    "    \"The relative error of NQS is: \",\n",
    "    jnp.absolute((energies_nqs[-1] - E_gs) / E_gs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05981c2c",
   "metadata": {},
   "source": [
    "Write a function to estimate the expectation value of the observable with Monte Carlo sampling (same as before for the energy but without the gradient). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3925f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def estimate_observable(model, parameters, obs, Ïƒ):\n",
    "    # reshape the samples, the samples are divided in different Markov chains\n",
    "    Ïƒ = Ïƒ.reshape(-1, Ïƒ.shape[-1])\n",
    "\n",
    "    O_loc = compute_eloc(model, parameters, obs, Ïƒ)\n",
    "\n",
    "    n_samples = O_loc.shape[0]\n",
    "\n",
    "    # compute the energy\n",
    "    O_average = jnp.mean(O_loc)\n",
    "    O_variance = jnp.var(O_loc)\n",
    "    O_error = jnp.std(O_loc) / jnp.sqrt(n_samples)\n",
    "    O = nk.stats.Stats(mean=O_average, error_of_mean=O_error, variance=O_variance)  # create an object containing the statistics\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2ed8e",
   "metadata": {},
   "source": [
    "We now propagate the state by solving the tVMC equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee27950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def compute_thetadot(model, params, H_jax, Ïƒ):\n",
    "    # compute grad and jacobians\n",
    "    E, E_grad, jacobian = ...\n",
    "\n",
    "    # convert from pytree to dense array\n",
    "    jacobian_dense = flatten_jacobian(jacobian)\n",
    "\n",
    "    # take the number of samples\n",
    "    n_samples = jacobian_dense.shape[0]\n",
    "\n",
    "    # center the jacobians\n",
    "    jacobian_centered = ...\n",
    "\n",
    "    # compute the S matrix\n",
    "    S = ...\n",
    "\n",
    "    # convert from pytree to dense array\n",
    "    F, unravel = nk.jax.tree_ravel(jax.tree.map(lambda x: -1j * x, E_grad))\n",
    "\n",
    "    # solve the linear system\n",
    "    thetadot, _ = ...         # nk.optimizer.solver.pinv_smooth\n",
    "\n",
    "    return unravel(thetadot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c17415",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')   # compile the function and make it faster to execute\n",
    "def RK4_scheme(model, params, H_jax, Ïƒ):\n",
    "    thetadot_1 = compute_thetadot(model, params, H_jax, Ïƒ)\n",
    "    params_1 = jax.tree.map(lambda x, y: x + (dt/2) * y, params, thetadot_1)\n",
    "\n",
    "    thetadot_2 = compute_thetadot(model, params_1, H_jax, Ïƒ)\n",
    "    params_2 = jax.tree.map(lambda x, y: x + (dt/2) * y, params, thetadot_2)\n",
    "\n",
    "    thetadot_3 = compute_thetadot(model, params_2, H_jax, Ïƒ)\n",
    "    params_3 = jax.tree.map(lambda x, y: x + dt * y, params, thetadot_3)\n",
    "    \n",
    "    thetadot_4 = compute_thetadot(model, params_3, H_jax, Ïƒ)\n",
    "\n",
    "    params = jax.tree.map(lambda a, x, y, w, z: a + (dt/6) * (x + 2*y + 2*w + z), params, thetadot_1, thetadot_2, thetadot_3, thetadot_4)   \n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observable_nqs = []\n",
    "\n",
    "# initialize the parameters\n",
    "params = params_v0\n",
    "\n",
    "# Initialize the sample\n",
    "sampler = nk.sampler.MetropolisLocal(\n",
    "    hi, \n",
    "    n_chains=16,    # we specify 16 chains\n",
    ")\n",
    "sampler_state = sampler.init_state(model, params, seed=1)\n",
    "\n",
    "# For every iteration\n",
    "for t in tqdm(ts):\n",
    "\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, params, sampler_state)\n",
    "    Ïƒ, sampler_state = sampler.sample(model, params, state=sampler_state, chain_length=50)\n",
    "\n",
    "    # update the parameters with a numerical scheme (e.g. Euler)\n",
    "    params = RK4_scheme(model, params, H_jax, Ïƒ)\n",
    "\n",
    "    # log the energy to a list\n",
    "    observable_nqs.append(estimate_observable(model, params, obs_jax, Ïƒ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5cd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts, jnp.array(obs_exact).real, linestyle='--', color='black', label=\"ED\")\n",
    "plt.plot(ts, [x.mean.real for x in observable_nqs], label=\"tVMC\")\n",
    "plt.xlabel(r\"$\\langle \\sigma_x(t) \\rangle$\")\n",
    "plt.ylabel(r\"$t$\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
