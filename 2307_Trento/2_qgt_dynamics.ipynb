{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a VMC code while computing the Ground-State of the Ising model: Part 2\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhilipVinc/Lectures/blob/master/2307_Trento/2_qgt_dynamics.ipynb) \n",
    "\n",
    "Let's get back to our dear old beautiful Ising hamiltonian\n",
    "\n",
    "$$ \n",
    "\\mathcal{H}=-h\\sum_{i}\\sigma_{i}^{(x)}+J\\sum_{\\langle i,j \\rangle}\\sigma_{i}^{(z)}\\sigma_{j}^{(z)}. \n",
    "$$\n",
    "\n",
    "In the following we assume periodic boundary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 0. Installing Netket\n",
    "\n",
    "If you are executing this notebook on Colab, you will need to install netket. You can do so by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet netket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want make to sure that this notebook is running on the cpu. \n",
    "You can edit the field by changing \"cpu\" to \"gpu\" to make it run on the GPU if you want. \n",
    "But you'll need to use much larger systems to see a benefit in the runtime.\n",
    "For systems with less than 40 spins GPUs slow you down remarkably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you are running on python at least 3.8, and that the installed NetKet version is at least 3.9.1. \n",
    "Note that this notebook won't run on Python versions prior to 3.8 and on NetKet versions prior to 3.9.1 .\n",
    "In general, conda might not have the most up-to-date version of netket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "import netket as nk\n",
    "\n",
    "print(\"Your version of python is: \", platform.python_version())\n",
    "print(\"Your version of netket is: \", nk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After* having defined this environment variable, we can load netket and the various libraries that we will be using throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import netket library\n",
    "import netket as nk\n",
    "\n",
    "# Import Json, this will be needed to load log files\n",
    "import json\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# jax and jax.numpy\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "#\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1 Define the Hamiltonians\n",
    "\n",
    "Let's consider as before an Ising model on a 2D lattice with periodic boundary conditions (pbc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a 2D square lattice\n",
    "L = 4\n",
    "g = nk.graph.Hypercube(length=L, n_dim=2, pbc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Hilbert space based on this graph\n",
    "hi = nk.hilbert.Spin(s=1/2, N=g.n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "hamiltonian = nk.operator.LocalOperator(hi) # This creates an empty operator to which you can add others.\n",
    "\n",
    "for site in g.nodes():\n",
    "    hamiltonian = hamiltonian - 1.0 * nk.operator.spin.sigmax(hi, site)\n",
    "\n",
    "for (i,j) in g.edges():\n",
    "    hamiltonian = hamiltonian + nk.operator.spin.sigmaz(hi, i)*nk.operator.spin.sigmaz(hi, j)\n",
    "\n",
    "hamiltonian_jax = hamiltonian.to_pauli_strings().to_jax_operator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2 Exact Diagonalization (as a testbed)\n",
    "\n",
    "Get the solution with exact diagonalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "e_gs, psi_gs = sp.sparse.linalg.eigsh( hamiltonian.to_sparse(), k=1)\n",
    "\n",
    "e_gs = e_gs[0]\n",
    "psi_gs = psi_gs[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.3 Variational Ansatz & variational energy\n",
    "\n",
    "We consider a Jastrow Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from netket.utils.types import DType, Array, NNInitFunc\n",
    "\n",
    "class Jastrow(nn.Module):\n",
    "    r\"\"\"\n",
    "    Jastrow wave function :math:`\\Psi(s) = \\exp(\\sum_{ij} s_i W_{ij} s_j)`.\n",
    "\n",
    "    The W matrix is stored as a non-symmetric matrix, and symmetrized\n",
    "    during computation by doing :code:`W = W + W.T` in the computation.\n",
    "    \"\"\"\n",
    "\n",
    "    param_dtype: DType = jnp.float64\n",
    "    \"\"\"The dtype of the weights.\"\"\"\n",
    "    kernel_init: NNInitFunc = jax.nn.initializers.normal()\n",
    "    \"\"\"Initializer for the weights.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x_in: Array):\n",
    "        nv = x_in.shape[-1]\n",
    "\n",
    "        kernel = self.param(\"kernel\", self.kernel_init, (nv, nv), self.param_dtype)\n",
    "        kernel = kernel + kernel.T\n",
    "\n",
    "        kernel, x_in = flax.linen.dtypes.promote_dtype(kernel, x_in, dtype=None)\n",
    "        y = jnp.einsum(\"...i,ij,...j\", x_in, kernel, x_in)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    r\"\"\"A restricted boltzman Machine, equivalent to a 2-layer FFNN with a\n",
    "    nonlinear activation function in between.\n",
    "    \"\"\"\n",
    "\n",
    "    param_dtype: DType = np.float64\n",
    "    \"\"\"The dtype of the weights.\"\"\"\n",
    "    activation: Any = nk.nn.reim_selu\n",
    "    \"\"\"The nonlinear activation function.\"\"\"\n",
    "    alpha: Union[float, int] = 1\n",
    "    \"\"\"feature density. Number of features equal to alpha * input.shape[-1]\"\"\"\n",
    "    use_hidden_bias: bool = True\n",
    "    \"\"\"if True uses a bias in the dense layer (hidden layer bias).\"\"\"\n",
    "    use_visible_bias: bool = True\n",
    "    \"\"\"if True adds a bias to the input not passed through the nonlinear layer.\"\"\"\n",
    "\n",
    "    kernel_init: NNInitFunc = jax.nn.initializers.normal(stddev=0.01)\n",
    "    \"\"\"Initializer for the Dense layer matrix.\"\"\"\n",
    "    hidden_bias_init: NNInitFunc = jax.nn.initializers.normal(stddev=0.01)\n",
    "    \"\"\"Initializer for the hidden bias.\"\"\"\n",
    "    visible_bias_init: NNInitFunc = jax.nn.initializers.normal(stddev=0.01)\n",
    "    \"\"\"Initializer for the visible bias.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input):\n",
    "        x = nn.Dense(\n",
    "            name=\"Dense\",\n",
    "            features=int(self.alpha * input.shape[-1]),\n",
    "            param_dtype=self.param_dtype,\n",
    "            use_bias=self.use_hidden_bias,\n",
    "            kernel_init=self.kernel_init,\n",
    "            bias_init=self.hidden_bias_init,\n",
    "        )(input)\n",
    "        x = self.activation(x)\n",
    "        x = jnp.sum(x, axis=-1)\n",
    "\n",
    "        if self.use_visible_bias:\n",
    "            v_bias = self.param(\n",
    "                \"visible_bias\",\n",
    "                self.visible_bias_init,\n",
    "                (input.shape[-1],),\n",
    "                self.param_dtype,\n",
    "            )\n",
    "            out_bias = jnp.dot(input, v_bias)\n",
    "            return x + out_bias\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.4. Utilities\n",
    "\n",
    "Those were the functions you (hopefully) wrote yesterday. Just copy pasted them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')\n",
    "def to_array(model, parameters):\n",
    "    # begin by generating all configurations in the hilbert space.\n",
    "    # all_States returns a batch of configurations that is (hi.n_states, N) large.\n",
    "    all_configurations = hi.all_states()\n",
    "\n",
    "    # now evaluate the model, and convert to a normalised wavefunction.\n",
    "    logpsi = model.apply(parameters, all_configurations)\n",
    "    psi = jnp.exp(logpsi)\n",
    "    psi = psi / jnp.linalg.norm(psi)\n",
    "    return psi\n",
    "\n",
    "@partial(jax.jit, static_argnames=\"model\")\n",
    "def compute_energy(model, parameters, hamiltonian_sparse):\n",
    "    psi_gs = to_array(model, parameters)\n",
    "    return psi_gs.conj().T@(hamiltonian_sparse@psi_gs)\n",
    "\n",
    "@partial(jax.jit, static_argnames='model')\n",
    "def compute_energy_and_gradient(model, parameters, hamiltonian_sparse):\n",
    "    grad_fun = jax.value_and_grad(compute_energy, argnums=1)\n",
    "    return grad_fun(model, parameters, hamiltonian_sparse)\n",
    "\n",
    "# MC\n",
    "@partial(jax.jit, static_argnames='model')\n",
    "def compute_local_energies(model, parameters, hamiltonian_jax, sigma):\n",
    "    eta, H_sigmaeta = hamiltonian_jax.get_conn_padded(sigma)\n",
    "    \n",
    "    logpsi_sigma = model.apply(parameters, sigma)\n",
    "    logpsi_eta = model.apply(parameters, eta)\n",
    "    logpsi_sigma = jnp.expand_dims(logpsi_sigma, -1) \n",
    "    \n",
    "    return jnp.sum(H_sigmaeta * jnp.exp(logpsi_eta - logpsi_sigma), axis=-1)\n",
    "\n",
    "@partial(jax.jit, static_argnames='model')\n",
    "def estimate_energy(model, parameters, hamiltonian_jax, sigma):\n",
    "    E_loc = compute_local_energies(model, parameters, hamiltonian_jax, sigma)\n",
    "    \n",
    "    E_average = jnp.mean(E_loc)\n",
    "    E_variance = jnp.var(E_loc)\n",
    "    E_error = jnp.sqrt(E_variance / E_loc.size)\n",
    "    \n",
    "    # we return a netket Stats object that wraps all statistical information related to this mean value.\n",
    "    return nk.stats.Stats(mean=E_average, error_of_mean=E_error, variance=E_variance)\n",
    "\n",
    "# @partial(jax.jit, static_argnames='model')\n",
    "def estimate_energy_and_gradient(model, parameters, hamiltonian_jax, sigma):\n",
    "    # reshape the samples to a vector of samples with no extra batch dimensions\n",
    "    sigma = sigma.reshape(-1, sigma.shape[-1])\n",
    "    \n",
    "    E_loc = compute_local_energies(model, parameters, hamiltonian_jax, sigma)\n",
    "    \n",
    "    # compute the energy as well\n",
    "    E_average = jnp.mean(E_loc)\n",
    "    E_variance = jnp.var(E_loc)\n",
    "    E_error = jnp.sqrt(E_variance/E_loc.size)\n",
    "    E = nk.stats.Stats(mean=E_average, error_of_mean=E_error, variance=E_variance)\n",
    "\n",
    "    # comptue the gradient ...\n",
    "    # first define the function to be differentiated\n",
    "    logpsi_sigma_fun = lambda pars : model.apply(pars, sigma)\n",
    "\n",
    "    # use jacrev with jax.tree_map, or even better, jax.vjp\n",
    "    _, vjpfun = jax.vjp(logpsi_sigma_fun, parameters)\n",
    "    E_grad = vjpfun((E_loc - E_average)/E_loc.size)\n",
    "\n",
    "    return E, E_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Gradient, aka Stochastic Reconfiguration\n",
    "\n",
    "Natural gradient descent uses information about the structure of the neural network to compute the 'gradient' according to the metric for the space of parameters (stretches and contracts euclidean space). This is what is called the Fubini-Study metric. Distances are represented by the angle between the vectors that represent the quantum states for parameters $\\theta$ and $\\theta '$:\n",
    "\n",
    "$$\n",
    "d(\\psi_\\theta, \\psi_{\\theta '}) = \\arccos \\left(|\\langle \\psi_\\theta|\\psi_{\\theta '} \\rangle | \\right)\n",
    "$$\n",
    "\n",
    "Using the good old trick of Taylor expanding things, we can find the infinitesimal squared line element:\n",
    "\n",
    "$$\n",
    "d^2(\\psi_\\theta, \\psi_{\\theta+\\delta\\theta}) = \\sum_{ij} g_{ij}(\\theta) \\delta \\theta_i \\delta \\theta_j,\n",
    "$$\n",
    "\n",
    "where $g_{ij}(\\theta)$ is the metric tensor derived from the Fubini-Study metric:\n",
    "\n",
    "$$\n",
    "g_{ij} = \\Re \\left[ \\left \\langle \\frac{  \\partial \\psi_\\theta}{\\partial \\theta_i} |\\frac{  \\partial \\psi_\\theta}{\\partial \\theta_j} \\right\\rangle - \\left \\langle \\frac{ \\partial \\psi_\\theta}{\\partial \\theta_i} |\\psi_\\theta \\right\\rangle \\left\\langle \\psi_\\theta| \\frac{  \\partial \\psi_\\theta}{\\partial \\theta_j} \\right\\rangle  \\right].\n",
    "$$\n",
    "\n",
    "Now that we have the infinitesimal squared line element, we can find the extremum of the variation of energy and distance in parameter space upon changing the parameters:\n",
    "\n",
    "$$\n",
    "L = E_{\\theta + \\delta \\theta} + \\lambda (d^2-\\epsilon) = E_\\theta + \\sum_k \\frac{\\partial E_\\theta}{\\partial\\theta_k} \\delta\\theta_k + \\lambda \\left( \\sum_{kk'} g_{kk'}(\\theta) \\delta \\theta_k \\delta \\theta_{k'} -\\epsilon \\right).\n",
    "$$\n",
    "\n",
    "The minimum of that function with respect to $\\delta \\theta$ is reached when $\\frac{\\partial L}{\\partial (\\delta\\theta_m)} = 0$:\n",
    "\n",
    "$$\n",
    "\\sum_{k'} g_{kk'}(\\theta) \\delta \\theta_{k'} = -\\frac{1}{2\\lambda} \\frac{\\partial E_\\theta}{\\partial \\theta_k},\n",
    "$$\n",
    "\n",
    "a system of linear equations that we need to solve (numerically). Please note the comparison of the update rule between natural and vanilla gradient descent:\n",
    "\n",
    "$$\n",
    "\\delta \\theta_k = -\\frac{1}{2\\lambda} \\frac{\\partial E_\\theta}{\\partial \\theta_k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Building the S matrix In practice : part 1, the jacobian\n",
    "\n",
    "You want to compute The Quantum Geometric Tensor\n",
    "\n",
    "$$\n",
    "S_{ij} = \\Re \\left[ \\left \\langle \\frac{  \\partial \\psi_\\theta}{\\partial \\theta_i} |\\frac{  \\partial \\psi_\\theta}{\\partial \\theta_j} \\right\\rangle - \\left \\langle \\frac{ \\partial \\psi_\\theta}{\\partial \\theta_i} |\\psi_\\theta \\right\\rangle \\left\\langle \\psi_\\theta| \\frac{  \\partial \\psi_\\theta}{\\partial \\theta_j} \\right\\rangle  \\right] \\\\\n",
    "= \\Re \\left[ \\sum_x \\frac{|\\psi(x)|^2}{\\langle\\psi|\\psi\\rangle} (\\partial_i \\log\\psi(x) - \\langle\\partial_i \\log\\psi \\rangle)^\\star (\\partial_j \\log\\psi(x) - \\langle\\partial_j \\log\\psi \\rangle) \\right] \n",
    "$$\n",
    "\n",
    "and the gradient of the energy\n",
    "\n",
    "$$\n",
    "f_i = \\text{as before}\n",
    "$$\n",
    "\n",
    "and update the parameters according to $\\theta_k$ which is computed by solving the linear system of equatins\n",
    "\n",
    "$$\n",
    "S_{ij}\\delta\\theta_j = f_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step one, computing the Quantum Geometric Tensor\n",
    "\n",
    "How can we easily get the Quantum Geometric tensor? If we could write everything in terms of a jacobian life would be a lot easier.\n",
    "\n",
    "So, there is a trick to abandom the world of pytrees and go to the world where all parameters are just a vector. It's a function called `jax.flatten_util.ravel_pytree`, which takes a pytree and returns a `vector` of concatenated parameters as well as a function to unpack the concatenated parameters in the original shape.\n",
    "\n",
    "See below an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Jastrow()\n",
    "parameters = model.init(jax.random.PRNGKey(1), jnp.ones(hi.size,))\n",
    "\n",
    "# we have parameters, a complicated object\n",
    "print(\"structure of parameters:\", jax.tree_map(lambda x: (x.shape, x.dtype), parameters))\n",
    "\n",
    "# Let's ravel it into a vector\n",
    "parameters_flat, unravel = jax.flatten_util.ravel_pytree(parameters)\n",
    "\n",
    "print(\"structure of parameters_flat:\", parameters_flat.shape, parameters_flat.dtype)\n",
    "\n",
    "# now i can modify the flattened parameters, for example by moltiplying them by 0\n",
    "parameters_flat_new = parameters_flat*0\n",
    "\n",
    "# unravel the new parameters\n",
    "unraveked_pars = unravel(parameters_flat_new)\n",
    "\n",
    "print(\"structure of new unraveled parameters:\", jax.tree_map(lambda x: (x.shape, x.dtype), unraveked_pars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can define a new function that takes as input the raveled parameters and returns the log wavefunction as usual. You could for example unflatten inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flattened_log_wavefunction(model, unravel_fun, flat_parameters, samples):\n",
    "    assert flat_parameters.ndim == 1\n",
    "    # todo\n",
    "    ...\n",
    "    res = model.apply(...)\n",
    "    ...\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "parameters_flat, unravel_fun = jax.flatten_util.ravel_pytree(parameters)\n",
    "samples = hi.random_state(jax.random.PRNGKey(3), (10,))\n",
    "\n",
    "assert flattened_log_wavefunction(model, unravel_fun, parameters_flat, samples).shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also compute the jacobian of this function with relative ease. This time, the jacobian should be a dense matrix. To do that, first build the callable that only takes as input the parameters then use `jax.jacrev` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# once your code works correctly, uncomment this line to jit it.\n",
    "# don't do it immediately because the error messages are harder to read then.\n",
    "@partial(jax.jit, static_argnames='model')\n",
    "def compute_jacobian(model, parameters, samples):\n",
    "    # enure we just have a long batch of samples\n",
    "    samples = samples.reshape(-1, samples.shape[-1])\n",
    "\n",
    "    parameters_flat, unravel_fun = jax.flatten_util.ravel_pytree(parameters)\n",
    "    def _fun(pars_flat, samples):\n",
    "        # todo\n",
    "        ...\n",
    "        return log_wavefunciton_for_samples\n",
    "    \n",
    "    # this line checks that the code works correctly. Check that it works then delete it\n",
    "    assert _fun(parameters_flat, samples).shape == (samples.shape[0],)\n",
    "    \n",
    "    def _compute_single_gradient(pars_flat, single_sample):\n",
    "        # only differentiate wrt to parameters, not samples\n",
    "        # compute the gradient of _fun... by using jax.grad\n",
    "        return ...\n",
    "    \n",
    "    # this line checks that the code works correctly. Check that it works then delete it\n",
    "    assert _compute_single_gradient(parameters_flat, samples[0]).shape == (parameters_flat.shape[0],)\n",
    "    \n",
    "    # vmap along samples (2nd argument) but not along parameters (1st argument)\n",
    "    _compute_gradient = jax.vmap(_compute_single_gradient, in_axes=(??, ??))\n",
    "    \n",
    "    # this line checks that the code works correctly. Check that it works then delete it\n",
    "    return _compute_gradient(parameters_flat, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert compute_jacobian(model, parameters, samples).shape == (samples.shape[0], nk.jax.tree_size(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Building the S matrix In practice : part 2, the S matrix itself\n",
    "\n",
    "Use this building block, the jacobian, and the function to compute the dense representation of the wavefunction to compute the full S matrix exactly, without sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @partial(jax.jit, static_argnames='model')\n",
    "def compute_S_matrix(model, parameters):\n",
    "    samples = hi.all_states()\n",
    "    \n",
    "    J = compute_jacobian(model, parameters, samples)\n",
    "    \n",
    "    # compute the vector of probability distribution p(samples)\n",
    "    # do not use to_array(), for additional speed, but reimplement it here.\n",
    "    probs = ...\n",
    "    \n",
    "    # now compute <d_k logpsi> = \\sum_i p_i J_ik\n",
    "    J_avg = ...\n",
    "    \n",
    "    # check\n",
    "    assert J_avg.shape == nk.jax.tree_size(parameters)\n",
    "    \n",
    "    # Now center the Jacobianby subtracting its average by cmputing J_ik - <J>_i\n",
    "    J_centered = J - ... # might be useful to jnp.expand_dims...\n",
    "    \n",
    "    # now compute the full S Matrix by averaging \n",
    "    S = ...\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "assert compute_S_matrix(model, parameters).shape == (nk.jax.tree_size(parameters), nk.jax.tree_size(parameters))\n",
    "\n",
    "# check hermitian\n",
    "np.testing.assert_allclose(compute_S_matrix(model, parameters), compute_S_matrix(model, parameters).conj().T)\n",
    "# check positive semidefeinite (approximately)\n",
    "np.all(jnp.linalg.eigh(compute_S_matrix(model, parameters))[0] > -1e-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Computing the natural gradient\n",
    "\n",
    "With this code, now we can try to build the linear system and solve it to compute the solution of a step!\n",
    "\n",
    "dtheta = solve(S, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')\n",
    "def compute_energy_and_natural_gradient(model, parameters, hamiltonian_jax_sparse):\n",
    "    \n",
    "    S_flat = compute_S_matrix(model, parameters)\n",
    "    \n",
    "    E, F = compute_energy_and_gradient(model, parameters, hamiltonian_jax_sparse)\n",
    "    \n",
    "    # Since S is a dense matrix, you must ravel F to something dense as well\n",
    "    F_flat = ...\n",
    "    \n",
    "    # Now S_flat and F_flat are both dense matrix/vector. You can use linear solvers to solve the linear\n",
    "    # system. Options are conjugate gradients (jax.scipy.sparse.linalg.cg) or similar methods\n",
    "    dtheta_flat = ...\n",
    "    \n",
    "    # dtheta is a dense vector, but the gradient should be a pytree. Unravel it back to the original structure\n",
    "    dtheta = ...\n",
    "    \n",
    "    return E, dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(compute_energy_and_natural_gradient(model, parameters, hamiltonian_jax.to_sparse())) == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "E, natgrad = compute_energy_and_natural_gradient(model, parameters, hamiltonian_jax.to_sparse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Finding the ground state with the natural gradient\n",
    "\n",
    "Now write a small training loop like we did yesterday using the Natural gradient instead of the standard gradient. Then, run both optimisations (using gradient and natural gradient) and compare the two.\n",
    "\n",
    "If you are using netket's loggers, use two different loggers, one called `logger_natgrad` and one called `logger_grad` for the two different optimisation runs.\n",
    "\n",
    "Try to initialise both runs with exactly the same parameters (use the same random seed) to see what is going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Jastrow()\n",
    "parameters = model.init(jax.random.PRNGKey(0), np.ones((hi.size, )))\n",
    "\n",
    "hamiltonian_jax_sparse = hamiltonian_jax.to_sparse()\n",
    "\n",
    "logger_grad = nk.logging.RuntimeLog()\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# same for natgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to plot the data, access it!\n",
    "plt.plot(logger_grad.data['Energy']['iters'], logger_grad.data['Energy']['value'])\n",
    "plt.plot(logger_natgrad.data['Energy']['iters'], logger_natgrad.data['Energy']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "But actually a better plot would be in log-scale, the relative error wrt the exact solution. Do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Natural gradient with MC sampling\n",
    "\n",
    "Until now we did things without sampling. Let's try to get it working with sampling now.\n",
    "\n",
    "The gradient `F` of the energy we have already with sampling thanks to the function `estimate_energy_and_gradient`. \n",
    "We just need to estimate the `S` matrix.\n",
    "\n",
    "The formula will be \n",
    "$$\n",
    "S_{ij} = = \\Re \\left[ \\frac{1}{N_s}\\sum_x  (\\partial_i \\log\\psi(x) - \\langle\\partial_i \\log\\psi \\rangle)^\\star (\\partial_j \\log\\psi(x) - \\langle\\partial_j \\log\\psi \\rangle) \\right] \n",
    "$$\n",
    "\n",
    "Luckily for us, we already have the code to compute the jacobian given a set of samples, so we just need to rewrite the code to patch it together and get the S matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jax.jit, static_argnames='model')\n",
    "def estimate_S_matrix(model, parameters, samples):\n",
    "     J = compute_jacobian(model, parameters, samples)\n",
    "    \n",
    "    # now compute <d_k logpsi> = 1/Ns \\sum_i J_ik\n",
    "    J_avg = ...\n",
    "    \n",
    "    # check\n",
    "    \n",
    "    # Now center the Jacobianby subtracting its average by cmputing J_ik - <J>_i\n",
    "    J_centered = J - ... # might be useful to jnp.expand_dims...\n",
    "    \n",
    "    # now compute the full S Matrix by summing over the batch dimension\n",
    "    S = ...\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake_samples = hi.random_state(jax.random.PRNGKey(0), (50,))\n",
    "\n",
    "assert estimate_S_matrix(model, parameters, fake_samples).shape == (nk.jax.tree_size(parameters), \n",
    "                                                                    nk.jax.tree_size(parameters))\n",
    "\n",
    "# check hermitian\n",
    "np.testing.assert_allclose(estimate_S_matrix(model, parameters, fake_samples), \n",
    "                           estimate_S_matrix(model, parameters, fake_samples).conj().T)\n",
    "# check positive semidefeinite (approximately)\n",
    "assert np.all(jnp.linalg.eigh(estimate_S_matrix(model, parameters, fake_samples))[0] > -1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now write the code to estimate the natural gradient using the estimation of the S matrix.\n",
    "This will be almost identical to the one you wrote for `compute_energy_and_natural_gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames='model')\n",
    "def estimate_energy_and_natural_gradient(model, parameters, hamiltonian_jax_sparse, samples):\n",
    "    \n",
    "    S_flat = compute_S_matrix(model, parameters, samples)\n",
    "    \n",
    "    E, F = estimate_energy_and_gradient(model, parameters, hamiltonian_jax_sparse, samples)\n",
    "    \n",
    "    # Since S is a dense matrix, you must ravel F to something dense as well\n",
    "    F_flat = ...\n",
    "    \n",
    "    # Now S_flat and F_flat are both dense matrix/vector. You can use linear solvers to solve the linear\n",
    "    # system. Options are conjugate gradients (jax.scipy.sparse.linalg.cg) or similar methods\n",
    "    dtheta_flat = ...\n",
    "    \n",
    "    # dtheta is a dense vector, but the gradient should be a pytree. Unravel it back to the original structure\n",
    "    dtheta = ...\n",
    "    \n",
    "    return E, dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use it in practice\n",
    "\n",
    "Run a calculation using Monte Carlo integration using an RBM neural network. \n",
    "Compare the result to the result you find with standard gradient descent.\n",
    "\n",
    "Which one is better?\n",
    "\n",
    "It's highly likely that the natural gradient optimisation will not be stable and will explode all the time. This is usually due to the fact that the S matrix, while technically positive definite, it no longer is due to sampling. The strategy usually employed is to regularise it by adding a small shift (1e-3) to its diagonal.\n",
    "\n",
    "If you have problems converging, try using this to stabilise it.\n",
    "\n",
    "Finally, try to compute the magnetization of the ground state that you found like that, compare against the exact solution. Which Network is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# settings \n",
    "model = RBM()\n",
    "sampler = nk.sampler.MetropolisSampler(\n",
    "                        hi,                            # the hilbert space to be sampled\n",
    "                        nk.sampler.rules.LocalRule(),  # the transition rule\n",
    "                        n_chains = 20)\n",
    "n_iters = 300\n",
    "chain_length = 1000//sampler.n_chains\n",
    "\n",
    "# initialise\n",
    "parameters = model.init(jax.random.PRNGKey(0), np.ones((hi.size, )))\n",
    "sampler_state = sampler.init_state(model, parameters, seed=1)\n",
    "\n",
    "# logging: you can (if you want) use netket loggers to avoid writing a lot of boilerplate...\n",
    "# they accumulate data you throw at them\n",
    "logger = nk.logging.RuntimeLog()\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "    # sample\n",
    "    sampler_state = sampler.reset(model, parameters, state=sampler_state)\n",
    "    samples, sampler_state = sampler.sample(model, parameters, state=sampler_state, chain_length=chain_length)\n",
    "    \n",
    "    # compute energy and gradient\n",
    "    E, E_grad = estimate_energy_and_natural_gradient(model, parameters, hamiltonian_jax, samples)\n",
    "    \n",
    "    # update parameters. Try using a learning rate of 0.01\n",
    "    ...\n",
    "    \n",
    "    # log energy: the logger takes a step argument and a dictionary of variables to be logged\n",
    "    logger(step=i, item={'Energy':E})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Symmetric Neural Network\n",
    "\n",
    "Let's consider the symmetries of this lattice. Luckily, netket is lovely and gives to us the translation group (as well as others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = g.translation_group()\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the way this object works is that if you apply it to samples, it gives you the translated samples. Look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's take one element of the hilbert space\n",
    "x = hi.numbers_to_states(400)\n",
    "print(x)\n",
    "\n",
    "# compute all element in the orbit of this state\n",
    "ys = G@x\n",
    "# it has shape (16,16) because the orbit contains 16 elements\n",
    "print(ys.shape)\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can build a Neural Network that uses this...\n",
    "\n",
    "$$\n",
    "        \\log\\psi_\\theta(\\sigma) = \\log\\sum_{g\\in G}\\chi_g\\exp[\\log\\psi_\\theta(T_{g}\\sigma)]\n",
    "$$\n",
    "\n",
    "where the base $\\log\\psi_\\theta$ is for example the standard RBM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "symm_group = g.translation_group()\n",
    "\n",
    "class SymmetrisedRBM(nn.Module):\n",
    "    non_symmetric_model : nn.Module\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array):\n",
    "        # apply the group and obtain a x_symm of shape (N_symm, ...)\n",
    "        x_symm = symm_group @ x\n",
    "        # reshape it to (-1, N_sites)\n",
    "        x_symm_shape = x_symm.shape\n",
    "        x_symm = x_symm.reshape(-1, x.shape[-1])\n",
    "\n",
    "        # Compute the log-wavefunction obtaining (-1,) and reshape to (N_symm, ...)\n",
    "        psi_symm = self.non_symmetric_model(x_symm).reshape(*x_symm_shape[:-1])\n",
    "\n",
    "        # log (sum_i ( c_i* exp(psi[sigma_i])))\n",
    "        psi = jax.scipy.special.logsumexp(psi_symm, axis=0)\n",
    "        return psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SymmetrisedRBM(non_symmetric_model=RBM())\n",
    "\n",
    "parameters = model.init(jax.random.PRNGKey(3), hi.numbers_to_states(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this model with SGD and Natural Gradient to find the ground state. Which works better? which has the best magnetization of the ground state? what about two body correlations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Netket development)",
   "language": "python",
   "name": "dev-netket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
