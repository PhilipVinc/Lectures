{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc1a161",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Toulouse School on Machine Learning in Quantum Many-Body Physics\n",
    "\n",
    "## Tutorial: Variational Monte Carlo with neural quantum states for the transverse-field Ising model\n",
    "\n",
    "Filippo Vicentini, Damian Hofmann, Giuseppe Carleo\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhilipVinc/Lectures/blob/main/22toulouse-jax-netket/2-nqs-ising.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae721781-2989-46c5-a92b-ca528b145e6c",
   "metadata": {},
   "source": [
    "In this Tutorial we will introduce the open-source package [NetKet](https://www.netket.org/), and show some of its functionalities. We will guide you through a relatively simple quantum problem, that however will be a good guide also to address more complex situations. \n",
    "\n",
    "Specifically, we will study the transverse-field Ising model in one dimension: \n",
    "\n",
    "$$ \n",
    "\\mathcal{H}=\\Gamma\\sum_{i}\\sigma_{i}^{(x)}+V\\sum_{i}\\sigma_{i}^{(z)}\\sigma_{i+1}^{(z)}. \n",
    "$$\n",
    "\n",
    "In the following we assume periodic boundary conditions and we will count lattice sites starting from $ 0 $, such that $ i=0,1\\dots L-1 $ and $i=L=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bddf19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Installing Netket \n",
    "\n",
    "If you are executing this notebook on Colab, you will need to install NetKet (and some other dependencies). You can do so by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640921c-e257-4735-8a02-fbd40891fe04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet netket==3.4 seaborn arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c26db4-9c04-45d5-ba78-f5e112b3e42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15de72d-9454-4873-a88f-a02ec10459ed",
   "metadata": {},
   "source": [
    "We also want make to sure that this notebook is running on the cpu. \n",
    "You can edit the field by changing \"cpu\" to \"gpu\" to make it run on the GPU if you want. \n",
    "But you'll need to use much larger systems to see a benefit in the runtime.\n",
    "For systems with less than 40 spins GPUs slow you down remarkably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752288d-caf4-4f53-a104-d4b2efe6e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb06b93-5fb0-47d5-be6d-d05f2e00c025",
   "metadata": {},
   "source": [
    "You can check that the installation was succesfull doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186ebe2-cddf-4324-a645-42c7f30cbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m netket.tools.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7effe",
   "metadata": {},
   "source": [
    "## 1. Defining the system and  Hamiltonian\n",
    "\n",
    "The first step in our journey consists in defining the Hamiltonian we are interested in. \n",
    "For this purpose, we first need to define the kind of degrees of freedom we are dealing with (i.e. if we have spins, bosons, fermions etc). \n",
    "This is done specifying the Hilbert space of the problem. For example, let us concentrate on a problem with 20 spins-1/2.\n",
    "\n",
    "When building hilbert spaces, in general, the first argument determines the size of the local basis and the latter defines how many modes you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk\n",
    "\n",
    "N = 20\n",
    "hi = nk.hilbert.Spin(s=1 / 2, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d889a00-c71a-450e-8ba5-af5c693706a5",
   "metadata": {},
   "source": [
    "NetKet's Hilbert spaces define the computational basis of the calculation, and are used to label and generate elements from it. \n",
    "The standard Spin-basis implicitly selects the `z` basis and elements of that basis will be elements $ v\\in\\{\\pm 1\\}^N $.\n",
    "\n",
    "It is possible to generate random basis elements through the function `random_state(rng, shape, dtype)`, where the first argument must be a jax RNG state (usually built with `jax.random.PRNGKey(seed)`, second is an integer or a tuple giving the shape of the samples and the last is the dtype of the generated states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bf39c-ae7f-4cb0-b141-093c448138e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "hi.random_state(jax.random.PRNGKey(0), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e67ac",
   "metadata": {},
   "source": [
    "We now need to specify the Hamiltonian. For this purpose, we will use NetKet's ```LocalOperator``` (see details [here](https://www.netket.org/docs/_generated/operator/netket.operator.LocalOperator.html#netket.operator.LocalOperator)) which is the sum of arbitrary k-local operators. \n",
    "\n",
    "In this specifc case, we have a 1-local operator, $ \\sigma^{(x)}_i $ and a 2-local operator, $ \\sigma^{(z)}_i \\sigma^{(z)}_j $. We then start importing the pauli operators. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netket.operator.spin import sigmax, sigmaz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c35665",
   "metadata": {},
   "source": [
    "We now take $ \\Gamma=-1 $ and start defining the 1-local parts of the Hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = -1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41d740",
   "metadata": {},
   "source": [
    "Here we have used a list comprehension to (mildly) show off our ability to write one-liners, however you could have just added the terms one by one in an explicit loop instead (though you'd end up with a whopping 3 lines of code). \n",
    "\n",
    "We now also add the interaction terms, using the fact that NetKet automatically recognizes products of local operators as tensor products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "V=-1\n",
    "H += sum([V*sigmaz(hi,i)*sigmaz(hi,(i+1)%N) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59025d8c-dede-496f-b2f0-6ad63966be1d",
   "metadata": {},
   "source": [
    "In general, when manipulating NetKet objects, you should always assume that you can safely operate on them like \n",
    "you would in mathematical equations, therefore you can sum and multiply them with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65581d8b",
   "metadata": {},
   "source": [
    "## 2. Exact Diagonalization\n",
    "\n",
    "Now that we have defined the Hamiltonian, we can already start playing with it. For example, since the number of spins is large but still manageable for exact diagonalization, we can give it a try. \n",
    "\n",
    "In NetKet this is easily done converting our Hamiltonian operator into a sparse matrix of size $ 2^N \\times 2^ N $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h=H.to_sparse()\n",
    "sp_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b786e6",
   "metadata": {},
   "source": [
    "Since this is just a regular scipy sparse matrix, we can just use any sparse diagonalization routine in there to find the eigenstates. For example, this will find the two lowest eigenstates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "eig_vals, eig_vecs = eigsh(sp_h, k=2, which=\"SA\")\n",
    "\n",
    "print(\"eigenvalues with scipy sparse:\", eig_vals)\n",
    "\n",
    "E_gs = eig_vals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51cc79",
   "metadata": {},
   "source": [
    "## 3. Mean-Field Ansatz\n",
    "\n",
    "We now would like to find a variational approximation of the ground state of this Hamiltonian. As a first step, we can try to use a very simple mean field ansatz: \n",
    "\n",
    "$$ \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{mf}} \\rangle = \\Pi_{i=1}^{N} \\Phi(\\sigma^{z}_i), $$\n",
    "\n",
    "where the variational parameters are the single-spin wave functions, which we can further take to be normalized: \n",
    "\n",
    "$$ |\\Phi(\\uparrow)|^2 + |\\Phi(\\downarrow)|^2 =1, $$\n",
    "\n",
    "and we can further write $ \\Phi(\\sigma^z) = \\sqrt{P(\\sigma^z)}e^{i \\phi(\\sigma^z)}$. In order to simplify the presentation, we take here and in the following examples the phase $ \\phi=0 $. In this specific model this is without loss of generality, since it is known that the ground state is real and positive. \n",
    "\n",
    "For the normalized single-spin probability we will take a sigmoid form: \n",
    "\n",
    "$$ P(\\sigma_z; \\lambda) = 1/(1+\\exp(-\\lambda \\sigma_z)), $$\n",
    "\n",
    "thus depending on the real-valued variational parameter $\\lambda$. \n",
    "In NetKet one has to define a variational function approximating the **logarithm** of the wave-function amplitudes (or density-matrix values).\n",
    "We call this variational function _the Model_ (yes, caps on the M).\n",
    "\n",
    "$$ \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{mf}} \\rangle = \\exp\\left[\\mathrm{Model}(\\sigma^{z}_1,\\dots \\sigma^{z}_N ; \\theta ) \\right], $$\n",
    "\n",
    "where $\\theta$ is a set of parameters. \n",
    "In this case, the parameter of the model will be just one: $\\gamma$.  \n",
    "\n",
    "The Model can be defined using one of the several *functional* jax frameworks such as Jax/Stax, Flax or Haiku. \n",
    "NetKet includes several pre-built models and layers built with [Flax](https://github.com/google/flax), so we will be using it for the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical operations in the model should always use jax.numpy \n",
    "# instead of numpy because jax supports computing derivatives. \n",
    "# If you want to better understand the difference between the two, check\n",
    "# https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Flax is a framework to define models using jax\n",
    "import flax\n",
    "# we refer to `flax.linen` as `nn`. It's a repository of \n",
    "# layers, initializers and nonlinear functions.\n",
    "import flax.linen as nn\n",
    "\n",
    "# A Flax model must be a class subclassing `nn.Module`\n",
    "class MF(nn.Module):\n",
    "    \n",
    "    # The most compact way to define the model is this.\n",
    "    # The __call__(self, x) function should take as \n",
    "    # input a batch of states x.shape = (n_samples, L)\n",
    "    # and should return a vector of n_samples log-amplitudes\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # A tensor of variational parameters is defined by calling\n",
    "        # the method `self.param` where the arguments will be:\n",
    "        # - arbitrary name used to refer to this set of parameters\n",
    "        # - an initializer used to provide the initial values. \n",
    "        # - The shape of the tensor\n",
    "        # - The dtype of the tensor.\n",
    "        lam = self.param(\n",
    "            \"lambda\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "        \n",
    "        # compute the probabilities\n",
    "        p = nn.log_sigmoid(lam*x)\n",
    "\n",
    "        # sum the output\n",
    "        return 0.5 * jnp.sum(p, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccb969-487e-43e6-ac87-50cde68c68e1",
   "metadata": {},
   "source": [
    "The model itself is only a set of instructions on how to initialise the parameters and how to compute the result. \n",
    "\n",
    "To actually create a variational state with its parameters, the easiest way is to construct a Monte-Carlo-sampled Variational State. \n",
    "To do this, we first need to define a sampler.\n",
    "\n",
    "In `netket.sampler` several samplers are defined, each with its own peculiarities. \n",
    "In the following example, we will be using a simple sampler that flips the spins in the configurations one by one.\n",
    "\n",
    "You can read more about how the sampler works by checking the documentation with `?nk.sampler.MetropolisLocal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb0ac3-bc4c-4743-adec-03bb4e2d7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model. \n",
    "# Notice that this does not create the parameters, but only the specification of how the model is constructed and acts upon inputs/.\n",
    "mf_model = MF()\n",
    "\n",
    "# Create the local sampler on the hilbert space\n",
    "sampler = nk.sampler.MetropolisLocal(hi, n_chains=8)\n",
    "\n",
    "# Construct the variational state using the model and the sampler above.\n",
    "# n_samples specifies how many samples should be used to compute expectation\n",
    "# values.\n",
    "vstate = nk.vqs.MCState(sampler, mf_model, n_samples=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9925a2-fbfb-4580-bba9-d8e233ab798c",
   "metadata": {},
   "source": [
    "You can play around with the variational state: for example, you can compute expectation values yourself or inspect it's parameters.\n",
    "The parameters are stored as a set of nested dictionaries. In this case, the single parameter $\\lambda$ is stored inside a (frozen) dictionary.\n",
    "(The reason why the dictionary is frozen is a detail of Flax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fae25-6b39-41c1-a25e-51de791b7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vstate.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f23399-7a3c-43c1-8d1c-14f8d5f13ea3",
   "metadata": {},
   "source": [
    "With a variational state, you can compute expectation values of operators using `vstate.expect`. For our `MCState`, this internally performs Monte Carlo sampling over `n_samples` spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea20c03-dcfa-4cda-8f2f-b1d86a95872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstate.n_samples = 4000\n",
    "E = vstate.expect(H)\n",
    "print(type(E))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e0116-93f7-4816-9b0a-85ebdd880bce",
   "metadata": {},
   "source": [
    "The return value of `.expect` is an object of type `Stats` which, beside the energy expectation value contains some statistics of the ensemble of local energies over which the energy is estimated.\n",
    "\n",
    "The `Stats` object also has a dictionary representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987ca3b-f4ec-4448-9304-27f3c2840350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(E.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703600e4-0df8-4b27-b3f4-aba1eb71d72e",
   "metadata": {},
   "source": [
    "The `Stats` object contains the following diagnostics:\n",
    "\n",
    "* The `Mean` over the local energy samples, which is an estimate of the quantum expectation value $\\langle \\hat H \\rangle.$\n",
    "\n",
    "* The `Variance` over the local energy samples, which is an estimate of the quantum variance $\\langle(\\delta\\hat H)^2\\rangle = \\langle (\\hat H - \\langle \\hat H \\rangle)^2 \\rangle.$\n",
    "\n",
    "* The Monte Carlo standard error (MCSE) of the mean (as `Sigma`).\n",
    "\n",
    "* An estimate `TauCorr` of the autocorrelation time over the Markov chains.\n",
    "\n",
    "* `R_hat`, which is the so-called Gelman-Rubin split-R_hat diagnostic, which indicates whether the MCMC chain is converged (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfba40-5626-45cd-8460-71077d23e15d",
   "metadata": {},
   "source": [
    "If you are close to an eigenstate of the operators, the variance should be close to 0. (In an exact eigenstate, the local energy is constant and therefore the variance is exactly zero.)\n",
    "\n",
    "The Gelman-Rubin diagnostic will be $\\hat{R}\\approx 1$ if the Markov chains are converged, while it will be larger than $1$ if your sampling has not converged.\n",
    "As a rule of thumb, look out for $|\\hat{R}| > 1.1$, and if that occurs consistently, check if you need more samples or if you MCMC scheme is even consistent with your system. (This is a somewhat weak criterion. Modern MCMC literature tends to recommend $|\\hat{R}| > 1.01$ to discard a sample. NQS optimization can still work with the less stringent criterion, but if you see stability problems, keep this in mind.)\n",
    "\n",
    "You can also investigate the correlation time of your estimator, $\\tau$. If $\\tau\\gg1$ then your samples are very correlated and you most likely have some issues with your sampling scheme.\n",
    "\n",
    "You can also access the fields individually:\n",
    "Note that if you run your calculation using MPI on different processes/machines, those estimators will return the mean, error and estimators of all the samples across all the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef403e6-7e3f-47df-8afe-b5cbc1a49695",
   "metadata": {},
   "source": [
    "### Interlude: Some notes on MCMC statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6db833-bd70-46fd-8270-80f8f3663c76",
   "metadata": {},
   "source": [
    "Lets look at how the statistics work a bit more closely. We'll be using some artifically generated random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25e787-deb1-4c3c-a6a8-5150948a6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some random values from a standard normal distribution\n",
    "xs = np.random.normal(size=1000)\n",
    "print(xs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fc309-adab-407f-b5f5-b8cd8d5775b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebee14a-be13-4137-9910-580faa7de36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a927627-1bfb-4514-a030-f809d35d85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=xs.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fffb3f-6a40-44fe-845f-f8965cb72edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.stats.statistics(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deceda49-2d29-45d9-9dce-c0cb72d7bcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa0f4e4-a7be-4798-bf08-5d11002bc179",
   "metadata": {},
   "source": [
    "How does the error of the mean scale with the number of samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab07bf-ed95-4e17-a5f1-22e566ea0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5 * 10**np.arange(1, 8)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee96fb0-01ab-4902-9f9b-45dc2f748e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcerrors = [nk.stats.statistics(np.random.normal(size=n)).error_of_mean for n in n_samples]\n",
    "plt.plot(n_samples, mcerrors)\n",
    "plt.plot(n_samples, 1/np.sqrt(n_samples), \"k--\")\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4245b46-1c4b-4d64-82ed-32e8ffb4204e",
   "metadata": {},
   "source": [
    "As is well known from MC theory, the error decreases proportionally to $1/\\sqrt{N_\\mathrm{samples}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586d112-d172-4979-8fed-ef49e4bb7042",
   "metadata": {},
   "source": [
    "But this is Markov chain Monte Carlo. How do we actually know whether our chain is already converged to the correct distribution? There could be insufficient burn-in or our chain is stuck in one part of the probability distribution and gives a biased estimate?\n",
    "\n",
    "Typical solution in MCMC: Run several independent chains (its more efficient anyways) and compare their estimates.\n",
    "\n",
    "For examples, let's generate several Gaussian \"chains\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b1571-6e7d-4cab-b90e-45a2338a7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chains(data):\n",
    "    _, axs = plt.subplots(1, data.shape[0], figsize=(16, 3), sharey=True)\n",
    "    for i, row in enumerate(data):\n",
    "        axs[i].plot(row)\n",
    "    plt.show()\n",
    "    sns.boxplot(data=data.T)\n",
    "    plt.show()\n",
    "    \n",
    "    stats = nk.stats.statistics(xs)\n",
    "    print(\"stats:         \", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35794fe-9a80-46ba-864f-ded747eb1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([\n",
    "    np.random.normal(size=500),\n",
    "    np.random.normal(size=500),\n",
    "    np.random.normal(size=500),\n",
    "    np.random.normal(size=500),\n",
    "])\n",
    "show_chains(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a6d68-6e29-46fe-8f7b-9d3cfe88fe3a",
   "metadata": {},
   "source": [
    "Let's see what happens if something goes wrong. Say, one chain disagrees with the others on the correct mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592376f3-8aa6-44d9-be6c-a3971de45b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = arviz.autocorr(xs)\n",
    "plt.plot(c.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8001b-ed17-42fd-afe9-4dfcc4745c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d8b3f-f1de-477a-9d22-8aab07b87f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000) + 2,\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "])\n",
    "show_chains(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217a991-5010-4b84-8435-f358eb453f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = arviz.autocorr(xs)\n",
    "plt.plot(c.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c424b8-4f5a-43f9-9551-11bc19cdf498",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "])\n",
    "xs[1, 200:] = xs[1, 200]\n",
    "xs[3, 200:] = xs[2, 200]\n",
    "show_chains(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d401e-3862-4155-a898-f0f4e05b6a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79525f46-9047-4d30-9ef9-479956bb3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = arviz.autocorr(xs)\n",
    "plt.plot(c.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e3714-a148-4bfc-b06c-4232053e5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "    np.random.normal(size=1000),\n",
    "])\n",
    "xs[:2] += 2*np.linspace(-1, 1, 1000)\n",
    "xs[2:] -= 2*np.linspace(-1, 1, 1000)\n",
    "show_chains(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af36703-48af-4362-a3a3-1630ad0a6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = arviz.autocorr(xs)\n",
    "plt.plot(c.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c35bf-1441-4b23-a02a-f330b49b6375",
   "metadata": {},
   "source": [
    "Getting back to our VMC example, let's try out a couple of `n_samples` values and see what happens to the energy statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55935ca0-92d6-4050-8475-41f643ce90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples   E\")\n",
    "for n_samples in [32, 64, 128, 512, 1024, 2048, 4096, 2**16, 2**17, 2**18]:\n",
    "    vstate.n_samples = n_samples\n",
    "    E = vstate.expect(H)\n",
    "    print(f\"{n_samples:>5}      {E}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc376f-033f-4d6b-9528-438275d8d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_many_chains(data, nx, ny):\n",
    "    _, axs = plt.subplots(nx, ny, figsize=(20, 6), sharex=True, sharey=True)\n",
    "    axs = axs.ravel()\n",
    "    for i, row in enumerate(data):\n",
    "        axs[i].plot(row)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    sns.boxplot(data=data.T)\n",
    "    plt.show()\n",
    "    \n",
    "    stats = nk.stats.statistics(data)\n",
    "    print(\"stats:         \", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb10588-9dca-4cf9-ab63-67647edf7f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac4cbb-60c9-426d-817c-4bde82670cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ca176-95a5-461b-a498-ec338af1dc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26fc52-2d06-45ef-98d6-6a6f6376c994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a851b5f-83dd-43eb-b9d4-cb22f20de381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta = vstate.variables\n",
    "\n",
    "@partial(jax.vmap, in_axes=0)\n",
    "@partial(jax.vmap, in_axes=0)\n",
    "def local_energy(sigma, sigmap, mels):\n",
    "    logpsi_s = mf_model.apply(theta, sigma)\n",
    "    def term(sp, mel):\n",
    "        return jnp.exp((mf_model.apply(theta, sp) - logpsi_s)) * mel\n",
    "    return jnp.sum(jax.vmap(term)(sigmap, mels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef06495-c060-4c48-99e9-3eb6413b7687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vstate.n_samples = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a759b7b-dc9c-4224-a0c2-53a23e1a789d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vstate.reset()\n",
    "s, (sp, mels) = nk.vqs.get_local_kernel_arguments(vstate, H)\n",
    "eloc = local_energy(s, sp, mels).T\n",
    "print(eloc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476590f7-209b-4d14-9657-8dcc2c317983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nk.stats.statistics(eloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e540c-e70a-45e3-a0d6-d42a9e01e8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_many_chains(eloc, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b299085-8052-4b42-8f3e-91a70d5296fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = arviz.autocorr(eloc)\n",
    "plt.plot(c.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b6302",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Variational Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab77267-892e-48d2-97e2-eee6c8eae796",
   "metadata": {},
   "source": [
    "We will now try to optimise $ \\lambda $ in order to best approximate the ground state of the hamiltonian.\n",
    "\n",
    "At first, we'll try to do this by ourself by writing the training loop, but then we'll switch to using a pre-made\n",
    "solution provided by netket for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103e357-f079-4816-8323-d9a606748a04",
   "metadata": {},
   "source": [
    "### 4a. DIY Optimisation loop\n",
    "\n",
    "The optimisation (or training) loop must do a very simple thing: at every iteration it must compute the energy and it's gradient, then multiply the gradient by a certain learning rate $\\lambda = 0.05$ and lastly it must update the parameters with this rescaled gradient.\n",
    "You can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b0687-1f78-4c2e-9f17-16c3fac2f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "energy_history = []\n",
    "n_steps = 100\n",
    "\n",
    "# For every iteration (tqdm is just a progress bar)\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # compute energy and gradient of the energy\n",
    "    E, E_grad = vstate.expect_and_grad(H)\n",
    "    # log the energy to a list\n",
    "    energy_history.append(E.mean.real)\n",
    "    # equivalent to vstate.parameters - 0.05*E_grad , but it performs this\n",
    "    # function on every leaf of the dictionaries containing the set of parameters\n",
    "    new_pars = jax.tree_multimap(lambda x,y: x-0.05*y, vstate.parameters, E_grad)\n",
    "    # actually update the paramters\n",
    "    vstate.parameters = new_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22ec7f-4479-47d4-a921-b568a4280c4f",
   "metadata": {},
   "source": [
    "We now can plot the energy during those optimisation steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f2a0e-3408-443d-8f9b-13f70db15231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(energy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ade91-7ad7-4a8c-997d-e4ee9098b2d7",
   "metadata": {},
   "source": [
    "### 4b. Use NetKet's optimisation driver \n",
    "\n",
    "As writing the whole optimisation loop by yourself every time is.. boring, we can make use of a coupled of NetKet's built-in utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we reset the parameters to run the optimisation again\n",
    "vstate.init_parameters()\n",
    "\n",
    "# Then we create an optimiser from the standard library.\n",
    "# You can also use optax.\n",
    "optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "\n",
    "# build the optimisation driver\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate)\n",
    "\n",
    "# run the driver for 300 iterations. This will display a progress bar\n",
    "# by default.\n",
    "gs.run(n_iter=300)\n",
    "\n",
    "mf_energy=vstate.expect(H)\n",
    "error=abs((mf_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",mf_energy,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b1a2d-fcbf-4248-ae18-d0f527ac67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also inspect the parameter:\n",
    "print(\"Final optimized parameter: \",vstate.parameters[\"lambda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5976b-da47-45af-88e8-690afb3980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0e656",
   "metadata": {},
   "source": [
    "## 5. Jastrow Ansatz\n",
    "\n",
    "We have seen that the mean field ansatz yields about 2% error on the ground-state energy. Let's now try to do better, using a more correlated ansatz. \n",
    "\n",
    "We will now take a short-range Jastrow ansatz, entangling nearest and next-to nearest neighbors, of the form \n",
    "\n",
    "$$ \\langle \\sigma^{z}_1,\\dots \\sigma^{z}_N| \\Psi_{\\mathrm{jas}} \\rangle = \\exp \\left( \\sum_i J_1 \\sigma^{z}_i\\sigma^{z}_{i+1} + J_2 \\sigma^{z}_i\\sigma^{z}_{i+2} \\right),$$\n",
    "\n",
    "where the parameters $J_1$ and $J_2$ are to be learned. \n",
    "\n",
    "Again we can write the model using flax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b390e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JasShort(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Define the two variational parameters J1 and J2\n",
    "        j1 = self.param(\n",
    "            \"j1\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "        j2 =self.param(\n",
    "            \"j2\", nn.initializers.normal(), (1,), float\n",
    "        )\n",
    "\n",
    "        # compute the nearest-neighbor correlations\n",
    "        corr1=x*jnp.roll(x,-1,axis=-1)\n",
    "        corr2=x*jnp.roll(x,-2,axis=-1)\n",
    "\n",
    "        # sum the output\n",
    "        return jnp.sum(j1*corr1+j2*corr2,axis=-1)\n",
    "    \n",
    "model=JasShort()\n",
    "\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=1008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616acc8",
   "metadata": {},
   "source": [
    "We then optimize it, however this time we also introduce a stochastic reconfiguration (natural gradient) preconditioner. Also, we now log the intermediate results of the optimization, so that we can visualize them at a later stage. \n",
    "\n",
    "Loggers that work together with optimisation drivers are defined in `nk.logging`. In this example we use `RuntimeLog`, which keeps the metrics in memory. You could also use `JsonLog`, which stores data to a json file which can be later read as a dict or `TensorBoardLog` which connects to [TensorBoard](https://www.tensorflow.org/tensorboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nk.optimizer.Sgd(learning_rate=0.05)\n",
    "\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate, preconditioner=nk.optimizer.SR(diag_shift=0.1))\n",
    "\n",
    "# construct the logger\n",
    "log=nk.logging.RuntimeLog()\n",
    "\n",
    "# One or more logger objects must be passed to the keyword argument `out`.\n",
    "gs.run(n_iter=300, out=log)\n",
    "\n",
    "print(f\"Final optimized parameters: j1={vstate.parameters['j1']}, j2={vstate.parameters['j2']}\")\n",
    "\n",
    "jas_energy=vstate.expect(H)\n",
    "error=abs((jas_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(f\"Optimized energy : {jas_energy}\")\n",
    "print(f\"relative error   : {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b18a6a",
   "metadata": {},
   "source": [
    "You can now see that this ansatz is almost one order of magnitude more accurate than the mean field! \n",
    "\n",
    "In order to visualize what happened during the optimization, we can use the data that has been stored by the logger. There are several available loggers in NetKet, here we have just used a simple one that stores the intermediate results as values in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34166d-92eb-4ad8-a43f-cffc3bb717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jastrow = log.data\n",
    "print(data_jastrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00670bd1",
   "metadata": {},
   "source": [
    "These report several intermediate quantities, that can be easily plotted. For example we can plot the value of the energy (with its error bar) at each optimization step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c22fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.errorbar(data_jastrow[\"Energy\"].iters, data_jastrow[\"Energy\"].Mean, yerr=data_jastrow[\"Energy\"].Sigma)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216a4d6",
   "metadata": {},
   "source": [
    "## 6. Neural Quantum State Ansatz\n",
    "\n",
    "We now want to use a more sofisticated ansatz, based on a neural network representation of the wave function. At this point, this is quite straightforward, since we can again take advantage of automatic differentiation. \n",
    "\n",
    "Let us define a simple fully-connected feed-forward network with a ReLu activation function and a sum layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    # You can define attributes at the module-level\n",
    "    # with a default. This allows you to easily change\n",
    "    # some hyper-parameter without redefining the whole \n",
    "    # flax module.\n",
    "    alpha : int = 1\n",
    "            \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # here we construct the first dense layer using a\n",
    "        # pre-built implementation in flax.\n",
    "        # features is the number of output nodes\n",
    "        # WARNING: Won't work with complex hamiltonians because\n",
    "        # of a bug in flax. Use nk.nn.Dense otherwise. \n",
    "        dense = nn.Dense(features=self.alpha * x.shape[-1])\n",
    "        \n",
    "        # we apply the dense layer to the input\n",
    "        y = dense(x)\n",
    "\n",
    "        # the non-linearity is a simple ReLu\n",
    "        y = nn.relu(y)\n",
    "                \n",
    "        # sum the output\n",
    "        return jnp.sum(y, axis=-1)\n",
    "    \n",
    "model = FFN(alpha=1)\n",
    "\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=1008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72882a6-57e8-4462-a333-24a500e71020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76a17a81-31cd-44c7-8e5a-d7d8743df8c7",
   "metadata": {},
   "source": [
    "*Warning*: Flax has a bug with its layers, where they drop the imaginary part\n",
    "of complex numbers if the layer has real weights.\n",
    "This is not a problem in the above example, but if you try to work on more complex\n",
    "problems where you work with complex numbers you should rather use the equivalent \n",
    "`nk.nn.Dense` which contains a fix for this bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29016f4",
   "metadata": {},
   "source": [
    "We then proceed to the optimization as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nk.optimizer.Sgd(learning_rate=0.1)\n",
    "\n",
    "# Notice the use, again of Stochastic Reconfiguration, which considerably improves the optimisation\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate,preconditioner=nk.optimizer.SR(diag_shift=0.1))\n",
    "\n",
    "log=nk.logging.RuntimeLog()\n",
    "gs.run(n_iter=300,out=log)\n",
    "\n",
    "ffn_energy=vstate.expect(H)\n",
    "error=abs((ffn_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",ffn_energy,error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26014bdc-f72b-466f-b7d7-761a7499ce25",
   "metadata": {},
   "source": [
    "And we can compare the results between the two different ansatze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10580934-c128-47ef-beb4-315420d63b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_FFN = log.data\n",
    "\n",
    "plt.errorbar(data_jastrow[\"Energy\"].iters, data_jastrow[\"Energy\"].Mean, yerr=data_jastrow[\"Energy\"].Sigma, label=\"Jastrow\")\n",
    "plt.errorbar(data_FFN[\"Energy\"].iters, data_FFN[\"Energy\"].Mean, yerr=data_FFN[\"Energy\"].Sigma, label=\"FFN\")\n",
    "plt.hlines([E_gs], xmin=0, xmax=300, color='black', label=\"Exact\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff286a6",
   "metadata": {},
   "source": [
    "## 7. Translation Symmetry \n",
    "\n",
    "In order to enforce spatial symmetries we can use some built-in functionalities of NetKet, in conjunction with equivariant layers. \n",
    "\n",
    "The first step is to construct explicitly a graph that contains the edges of our interactions, in this case this is a simple chain with periodic boundaries. NetKet has builtin several symmetry groups that can be used to target specific spatial symmetries. In this case for example after constructing the graph we can also print its translation group. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f147d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=nk.graph.Chain(length=N, pbc=True)\n",
    "\n",
    "print(graph.translation_group())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cabe68",
   "metadata": {},
   "source": [
    "Graphs are in general quite handy when defining hamiltonian terms on their edges. For example we can define our Hamiltonian as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma=-1\n",
    "H = sum([Gamma*sigmax(hi,i) for i in range(N)])\n",
    "\n",
    "V=-1\n",
    "H += sum([V*sigmaz(hi,i)*sigmaz(hi,j) for (i,j) in graph.edges()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0196f9f",
   "metadata": {},
   "source": [
    "We now write a model with an invariant transformation given by the translation group. Notice that we will now use NetKet's own ```nn``` module, instead of Flax, since it contains several additions and also an extended and seamless support for complex layers/parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f65b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket.nn as nknn\n",
    "\n",
    "class SymmModel(nn.Module):\n",
    "    alpha: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # add an extra dimension with size 1, because DenseSymm requires rank-3 tensors as inputs.\n",
    "        # the shape will now be (batches, 1, Nsites)\n",
    "        x = x.reshape(-1, 1, x.shape[-1])\n",
    "        \n",
    "        x = nknn.DenseSymm(symmetries=graph.translation_group(),\n",
    "                           features=self.alpha,\n",
    "                           kernel_init=nn.initializers.normal(stddev=0.01))(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # sum the output\n",
    "        return jnp.sum(x,axis=(-1,-2))\n",
    "\n",
    "\n",
    "sampler = nk.sampler.MetropolisLocal(hi)\n",
    "\n",
    "#Let us define a model with 4 features per symmetry\n",
    "model=SymmModel(alpha=4)\n",
    "\n",
    "vstate = nk.vqs.MCState(sampler, model, n_samples=1008)\n",
    "\n",
    "vstate.n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ca35d",
   "metadata": {},
   "source": [
    "As it can be seen, the number of parameters of this model is greatly reduced, because of the symmetries that impose constraints on the weights of the dense layers. We can now optimize the model, using a few more optimization steps than before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a45cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nk.optimizer.Sgd(learning_rate=0.1)\n",
    "\n",
    "gs = nk.driver.VMC(H, optimizer, variational_state=vstate,preconditioner=nk.optimizer.SR(diag_shift=0.1))\n",
    "\n",
    "log=nk.logging.RuntimeLog()\n",
    "gs.run(n_iter=600,out=log)\n",
    "\n",
    "symm_energy=vstate.expect(H)\n",
    "error=abs((symm_energy.mean-eig_vals[0])/eig_vals[0])\n",
    "print(\"Optimized energy and relative error: \",symm_energy,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ed895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(log.data[\"Energy\"].iters[50:],log.data[\"Energy\"].Mean[50:],yerr=log.data[\"Energy\"].Sigma[50:],label=\"SymmModel\")\n",
    "\n",
    "plt.axhline(y=eig_vals[0], xmin=0, xmax=log.data[\"Energy\"].iters[-1], linewidth=2, color=\"k\", label=\"Exact\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Energy')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f501c7",
   "metadata": {},
   "source": [
    "## 8. Measuring Other Properties\n",
    "\n",
    "Once the model has been optimized, we can of course measure also other observables that are not the energy. For example, we could decide to measure the value of the nearest-neighbor $X-X$ correlator. \n",
    "Notice that since correlators do not enjoy the zero-variance principle as the Hamiltonian instead does, it is important to use a larger number of samples to have a sufficiently low error bar on their measurement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = sum([sigmax(hi,i)*sigmax(hi,j) for (i,j) in graph.edges()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstate.n_samples=400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstate.expect(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daae42d",
   "metadata": {},
   "source": [
    "And we can further compare this to the exact ED result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = eig_vecs[:, 0]\n",
    "exact_corr=psi@(corr@psi)\n",
    "print(exact_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5ebd2-5e85-4c44-9948-9ee54ffcafda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('python-3.8.2': venv)",
   "language": "python",
   "name": "python38264bitpython382venve5ba4f1b897945f7bd42c4f3d1c19160"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
